{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facba287",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Sequence\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.patches as patches\n",
    "from omegaconf import OmegaConf\n",
    "import hydra\n",
    "from hydra import initialize_config_dir, compose\n",
    "import time\n",
    "import imageio\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import spnf\n",
    "from spnf.utils import set_seed, make_coord_grid, apply_to_tensors, to_py\n",
    "from spnf.sample import (\n",
    "    rand_ortho, logrand, construct_covariance, sample_gaussian_delta, sample_ellipsoid_delta\n",
    ")\n",
    "from spnf.trainer import Trainer\n",
    "\n",
    "config_dir = Path(spnf.__file__).parent / \"configs\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def get_cfg(config_name, config_dir: Path, overrides: list):\n",
    "    with initialize_config_dir(version_base=None, config_dir=str(config_dir)):\n",
    "        cfg = compose(config_name=config_name, overrides=overrides)\n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6151715",
   "metadata": {},
   "source": [
    "## Training visualization for neural fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a04282",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_decay_start = 500\n",
    "vis_steps = 800\n",
    "total_steps = 2000\n",
    "SRC_RES = 512\n",
    "FPS = 30\n",
    "cfg = get_cfg(\n",
    "    config_name=\"train\",\n",
    "    config_dir=config_dir,\n",
    "    overrides=[\n",
    "        f\"data.grid.resize={SRC_RES}\",\n",
    "        \"data.bounds=1.0\", \n",
    "        \"tensorboard=null\",\n",
    "        f\"trainer.steps={total_steps}\",\n",
    "        \"paths.output_dir=/home/myaldiz/Data/Experiments/spnf/${task_name}\",\n",
    "        f\"scheduler.lr_lambda.decay_start={lambda_decay_start}\",\n",
    "        \"trainer.compile_train_step=False\",\n",
    "        \"trainer.mc_samples_train=0\"\n",
    "    ],\n",
    ")\n",
    "video_settings = dict(\n",
    "    fps=FPS,\n",
    "    codec='libx265', \n",
    "    pixelformat='yuv420p',\n",
    "    ffmpeg_params=[\n",
    "        '-crf', '10',          # Lower CRF = Higher Quality (10 is extremely high quality)\n",
    "        '-preset', 'veryslow', # Takes longer to render, but best compression\n",
    "        '-tag:v', 'hvc1',      # Critical for Apple compatibility\n",
    "        '-tune', 'grain',      # Preserves noise/aliasing details\n",
    "        '-x265-params', 'keyint=30:bframes=0' # Advanced Tweaks (see below)\n",
    "    ]\n",
    ")\n",
    "dev_video_settings = video_settings | dict(\n",
    "    ffmpeg_params=[\n",
    "        '-crf', '20', \n",
    "        '-preset', 'fast',\n",
    "        '-tag:v', 'hvc1',\n",
    "        '-tune', 'grain',\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create output directories\n",
    "output_dir = Path(cfg.paths.output_dir)\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "checkpoint_dir = Path(cfg.trainer.checkpoint_dir)\n",
    "checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "video_output_dir = output_dir / \"visualizations\" \n",
    "video_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save the config to output directory\n",
    "OmegaConf.save(config=cfg, f=output_dir / \"config_singlescale.yaml\", resolve=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fe1f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "VIS_PATCH_RES = 64\n",
    "vis_patch_x, vis_patch_y = 190, 120\n",
    "trainer = Trainer(cfg).to(device)\n",
    "\n",
    "loss_average = []\n",
    "predictions = []\n",
    "steps_history = []  # New: Track the actual step number for the X-axis\n",
    "\n",
    "current_step = 0\n",
    "frame_count = 0\n",
    "pbar = tqdm(total=vis_steps, desc=\"Training\")\n",
    "\n",
    "while current_step < vis_steps:\n",
    "    # Determine stride based on how many frames we have generated\n",
    "    if current_step < 90:\n",
    "        stride = 1\n",
    "    elif current_step < 210:\n",
    "        stride = 2\n",
    "    else:\n",
    "        stride = 5\n",
    "        \n",
    "    # Ensure we don't exceed vis_steps\n",
    "    if current_step + stride > vis_steps:\n",
    "        stride = vis_steps - current_step\n",
    "        if stride == 0: break\n",
    "\n",
    "    # Fit for 'stride' steps\n",
    "    stats = trainer.fit(num_steps=stride, no_tqdm=True)\n",
    "    \n",
    "    # Update trackers\n",
    "    current_step += stride\n",
    "    frame_count += 1\n",
    "    pbar.update(stride)\n",
    "\n",
    "    # Store Data\n",
    "    loss_average.append(np.mean(stats['mse_loss']))\n",
    "    steps_history.append(current_step) # Store actual X-axis location\n",
    "    \n",
    "    # Generate visualization\n",
    "    output = trainer.generate_grid_data(resolution=SRC_RES)\n",
    "    predictions.append(output[\"filtered_signal\"].clamp(-1.0, 1.0).add(1.0).div(2.0).cpu().numpy())\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "# --- VIDEO GENERATION ---\n",
    "video_output = output_dir / \"visualizations\" / \"singlescale_train_vis.mov\"\n",
    "video_patch_output = output_dir / \"visualizations\" / \"singlescale_train_vis_patch.mov\"\n",
    "writer = imageio.get_writer(\n",
    "    video_output, \n",
    "    **video_settings\n",
    ")\n",
    "writer_patch = imageio.get_writer(\n",
    "    video_patch_output, \n",
    "    **video_settings\n",
    ")\n",
    "\n",
    "# Settings\n",
    "plt.style.use('default') \n",
    "max_loss = max(loss_average) * 1.1\n",
    "\n",
    "# 1. Geometry Calculation\n",
    "sample = predictions[0]\n",
    "if sample.ndim == 3 and sample.shape[0] in [1, 3]:\n",
    "    h, w = sample.shape[1], sample.shape[2]\n",
    "else:\n",
    "    h, w = sample.shape[0], sample.shape[1]\n",
    "\n",
    "fig_width = 12 \n",
    "img_height_in = fig_width * (h / w)\n",
    "loss_height_in = 4.0 \n",
    "total_height = img_height_in + loss_height_in\n",
    "\n",
    "# Calculate relative height ratios for manual positioning\n",
    "h_img_ratio = img_height_in / total_height\n",
    "h_loss_ratio = loss_height_in / total_height\n",
    "\n",
    "print(\"Rendering video frames...\")\n",
    "# Zip with steps_history to get the correct X-axis value\n",
    "for i, (loss, pred, step_num) in enumerate(tqdm(zip(loss_average, predictions, steps_history), total=len(predictions))):\n",
    "    \n",
    "    # Create figure without subplots/layouts to allow manual placement\n",
    "    fig = plt.figure(figsize=(fig_width, total_height), dpi=100)\n",
    "    \n",
    "    # Bottom Plot\n",
    "    margin_bottom = 0.08\n",
    "    h_loss_actual = h_loss_ratio - margin_bottom - 0.02\n",
    "    ax_loss = fig.add_axes([0.12, margin_bottom, 0.82, h_loss_actual])\n",
    "    \n",
    "    margin_top = 0.02\n",
    "    y_img_start = h_loss_ratio + 0.02\n",
    "    h_img_actual = h_img_ratio - margin_top - 0.02\n",
    "    ax_img = fig.add_axes([0.02, y_img_start, 0.96, h_img_actual])\n",
    "\n",
    "    # --- 1. Image Plot ---\n",
    "    if pred.ndim == 3 and pred.shape[0] in [1, 3]: \n",
    "        pred = np.transpose(pred, (1, 2, 0))\n",
    "    \n",
    "    ax_img.imshow(pred, aspect='auto') \n",
    "    \n",
    "    # Hide all ticks/spines for the image\n",
    "    ax_img.set_xticks([])\n",
    "    ax_img.set_yticks([])\n",
    "    for spine in ax_img.spines.values():\n",
    "        spine.set_visible(False) \n",
    "    \n",
    "    # --- 2. Loss Plot ---\n",
    "    # MODIFICATION: Use steps_history for X-axis data\n",
    "    current_steps_data = steps_history[:i+1]\n",
    "    current_loss_data = loss_average[:i+1]\n",
    "    \n",
    "    ax_loss.plot(current_steps_data, current_loss_data, color='tab:blue', linewidth=3)\n",
    "    \n",
    "    # MODIFICATION: Use step_num (the actual training step) for the scatter X-coordinate\n",
    "    ax_loss.scatter(step_num, loss, color='tab:red', s=100, zorder=5) \n",
    "    \n",
    "    # MODIFICATION: Set X limit to the total vis_steps (800) rather than frame count\n",
    "    ax_loss.set_xlim(0, vis_steps)\n",
    "    ax_loss.set_ylim(0, max_loss)\n",
    "    \n",
    "    ax_loss.set_ylabel('MSE Loss', fontsize=24, labelpad=15, fontweight='medium')\n",
    "    ax_loss.set_xlabel('Steps', fontsize=18) # Optional: Label x-axis\n",
    "    ax_loss.tick_params(axis='both', which='major', labelsize=16)\n",
    "    ax_loss.grid(True, linestyle='--', alpha=0.5, linewidth=1.5)\n",
    "\n",
    "    # --- 3. Save Frame ---\n",
    "    fig.canvas.draw()\n",
    "    frame = np.array(fig.canvas.buffer_rgba())\n",
    "    \n",
    "    writer.append_data(frame)\n",
    "    patch = pred[vis_patch_y:vis_patch_y+VIS_PATCH_RES, vis_patch_x:vis_patch_x+VIS_PATCH_RES]\n",
    "    \n",
    "    writer_patch.append_data((patch * 255).astype(np.uint8))\n",
    "    plt.close(fig)\n",
    "\n",
    "writer.close()\n",
    "writer_patch.close()\n",
    "print(f\"Videos are saved: {video_output}, {video_patch_output}\")\n",
    "\n",
    "# Save last frame patch as PNG\n",
    "last_frame_output = output_dir / \"visualizations\" / \"singlescale_train_vis_patch.png\"\n",
    "patch = predictions[-1][vis_patch_y:vis_patch_y+VIS_PATCH_RES, vis_patch_x:vis_patch_x+VIS_PATCH_RES]\n",
    "imageio.imwrite(\n",
    "    last_frame_output, \n",
    "    (patch * 255).astype(np.uint8)\n",
    ")\n",
    "print(f\"Last frame image saved: {last_frame_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8054ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train remaining steps\n",
    "trainer.fit(num_steps=total_steps - vis_steps)\n",
    "\n",
    "# Save the state dict\n",
    "state_dict = trainer.state_dict()\n",
    "checkpoint_path = checkpoint_dir / \"singlescale.pth\"\n",
    "torch.save(state_dict, str(checkpoint_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb6ca53",
   "metadata": {},
   "source": [
    "## Visualize aliasing artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a2856b",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = checkpoint_dir / \"singlescale.pth\"\n",
    "if not checkpoint_path.exists():\n",
    "    # Fit the model\n",
    "    trainer.fit()\n",
    "\n",
    "    # Save the state dict\n",
    "    state_dict = trainer.state_dict()\n",
    "    torch.save(state_dict, str(checkpoint_path))\n",
    "else:\n",
    "    # Load the state dict\n",
    "    trainer = Trainer(cfg).to(device)\n",
    "    state_dict = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "    trainer.load_state_dict(state_dict)\n",
    "    print(f\"Loaded model from {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d69de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ Visualization Script ============\n",
    "COLOR_BOX = '#D46CCD'\n",
    "COLOR_POINTS = '#4E71BE'\n",
    "SAMPLE_RES = 32\n",
    "FPS = 30\n",
    "SECONDS = 12\n",
    "FRAMES = FPS * SECONDS\n",
    "video_output = output_dir / \"visualizations\" / \"singlescale_aliasing_vis.mov\"\n",
    "\n",
    "# --- Keyframes (Fixed Timing) ---\n",
    "# Format: (Time_Percent, Center_X, Center_Y, Span)\n",
    "keyframes = [\n",
    "    (0.00, -0.5,  0.5, 0.65), \n",
    "    (0.40,  0.5,  0.5, 0.65), \n",
    "    (0.50,  0.5, -0.4, 0.95),\n",
    "    (0.90, -0.5, -0.4, 0.95), \n",
    "    (1.00, -0.5,  0.5, 0.65),\n",
    "]\n",
    "\n",
    "def get_trajectory_point_time(t_global, keyframes):\n",
    "    for k in range(len(keyframes) - 1):\n",
    "        t0, x0, y0, s0 = keyframes[k]\n",
    "        t1, x1, y1, s1 = keyframes[k+1]\n",
    "        if t0 <= t_global <= t1 + 1e-5:\n",
    "            segment_duration = t1 - t0\n",
    "            if segment_duration <= 1e-5: return x1, y1, s1\n",
    "            local_t = np.clip((t_global - t0) / segment_duration, 0, 1)\n",
    "            smooth_t = local_t * local_t * (3 - 2 * local_t) \n",
    "            current_x = x0 + (x1 - x0) * smooth_t\n",
    "            current_y = y0 + (y1 - y0) * smooth_t\n",
    "            current_s = s0 + (s1 - s0) * smooth_t\n",
    "            return current_x, current_y, current_s\n",
    "    return keyframes[-1][1:]\n",
    "\n",
    "def clamp_bounds(cx, cy, span, limit=1.0):\n",
    "    half_s = span / 2.0\n",
    "    if cx - half_s < -limit: cx = -limit + half_s\n",
    "    if cx + half_s > limit:  cx = limit - half_s\n",
    "    if cy - half_s < -limit: cy = -limit + half_s\n",
    "    if cy + half_s > limit:  cy = limit - half_s\n",
    "    return cx, cy\n",
    "\n",
    "# --- Setup Plot ---\n",
    "plt.style.use('seaborn-v0_8-white') \n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5.5), dpi=200, constrained_layout=True)\n",
    "fig.patch.set_facecolor('white')\n",
    "\n",
    "# Context View\n",
    "full_field = trainer.data\n",
    "output = trainer.generate_grid_data(resolution=SRC_RES)\n",
    "reconstructed_data = output[\"filtered_signal\"].clamp(-1.0, 1.0).add(1.0).div(2.0).cpu().numpy()\n",
    "im_context = axs[0].imshow(reconstructed_data, extent=(-1, 1, 1, -1), cmap='gray', vmin=0, vmax=1)\n",
    "axs[0].set_title(\"Learned Signal (Neural Field)\", fontsize=14, fontweight='bold', color='gray')\n",
    "axs[0].axis('off')\n",
    "\n",
    "rect = patches.Rectangle((0,0), 0, 0, linewidth=2.5, edgecolor=COLOR_BOX, facecolor='none')\n",
    "axs[0].add_patch(rect)\n",
    "\n",
    "# FIX 1: Smaller points (s=4) and alpha=0.6 for better visibility without occlusion\n",
    "scat = axs[0].scatter([], [], s=4, c=COLOR_POINTS, edgecolors='none', alpha=0.6)\n",
    "\n",
    "# Naive & Ours Views\n",
    "dummy_data = np.zeros((SAMPLE_RES, SAMPLE_RES, 3))\n",
    "im_naive = axs[1].imshow(dummy_data, vmin=0, vmax=1, interpolation='nearest') \n",
    "axs[1].set_title(\"Naive Sampling (Aliased)\", fontsize=14, fontweight='bold', color=COLOR_BOX)\n",
    "axs[1].axis('off')\n",
    "\n",
    "# --- Render Loop ---\n",
    "writer = imageio.get_writer(\n",
    "    video_output, \n",
    "    **video_settings,\n",
    ")\n",
    "\n",
    "print(f\"Rendering {FRAMES} frames...\")\n",
    "\n",
    "for i in tqdm(range(FRAMES)):\n",
    "    t = i / (FRAMES - 1)\n",
    "    \n",
    "    # Logic\n",
    "    cx, cy, span = get_trajectory_point_time(t, keyframes)\n",
    "    cx, cy = clamp_bounds(cx, cy, span, limit=0.99)\n",
    "    bounds = (cx - span/2, cy - span/2, cx + span/2, cy + span/2)\n",
    "    coords = make_coord_grid(ndim=2, resolution=SAMPLE_RES, bounds=bounds, device=device)\n",
    "\n",
    "    # Naive\n",
    "    with torch.no_grad():\n",
    "        naive_out = trainer.model.forward(\n",
    "            {\"coords\": coords.view(-1, 2)},\n",
    "        )\n",
    "        naive_out = naive_out[\"filtered_signal\"].view(SAMPLE_RES, SAMPLE_RES, 3)\n",
    "        naive_out = naive_out.clamp(-1.0, 1.0).add(1.0).div(2.0).cpu().numpy()\n",
    "\n",
    "    # Updates\n",
    "    rect.set_xy((bounds[0], bounds[1])) \n",
    "    rect.set_width(span)\n",
    "    rect.set_height(span)\n",
    "    \n",
    "    c_np = coords.view(-1,2).cpu().numpy()\n",
    "    scat.set_offsets(c_np)\n",
    "    \n",
    "    # Image Updates\n",
    "    im_naive.set_data(naive_out)\n",
    "    \n",
    "    fig.canvas.draw()\n",
    "    frame = np.asarray(fig.canvas.buffer_rgba())\n",
    "    writer.append_data(frame)\n",
    "\n",
    "writer.close()\n",
    "plt.close()\n",
    "print(f\"Done. Saved {video_output}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spnf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
