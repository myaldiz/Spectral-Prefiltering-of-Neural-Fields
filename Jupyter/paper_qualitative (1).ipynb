{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7946/2847699351.py:38: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  anisotropic_paper = torch.load(ngssf.data._data_dir / \"covariance_matrices\" / \"3d.pt\").to(device)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import hydra\n",
    "from hydra import initialize_config_dir, compose, initialize\n",
    "import torch\n",
    "import lightning as L\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import ngssf\n",
    "import cv2\n",
    "import open3d as o3d\n",
    "import trimesh\n",
    "from tqdm.notebook import tqdm\n",
    "from ngssf.sampler import FieldDataGenerator\n",
    "from ngssf.util import _spectrum, VideoEncoderThread\n",
    "\n",
    "sys.path.append(\"/Code/viscomp_reconstruction/ngssf/scripts\")\n",
    "from train_hydra import Trainer\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "# Variables\n",
    "# initialize(version_base=None, config_path=\"../configs\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# -------------------------------------------------------------------------------\n",
    "# Functions to load the model\n",
    "def get_cfg_from_experiment(experiment_path: Path, overrides: list):\n",
    "    # Point to the .hydra folder you used during training\n",
    "    hydra_cfg_dir = str(experiment_path / \".hydra\")\n",
    "\n",
    "    with initialize_config_dir(version_base=None, config_dir=hydra_cfg_dir):\n",
    "        # config_name is the filename without “.yaml”\n",
    "        cfg = compose(config_name=\"config\", overrides=overrides)\n",
    "    return cfg\n",
    "# -------------------------------------------------------------------------------\n",
    "# Load the kernels for visualizations\n",
    "# Load anisotropic kernels from the paper\n",
    "anisotropic_paper = torch.load(ngssf.data._data_dir / \"covariance_matrices\" / \"3d.pt\").to(device)\n",
    "# anisotropic_paper = torch.load(ngssf.data._data_dir / \"covariance_matrices\" / \"2d_vis.pt\")['video'].to(device)\n",
    "\n",
    "# # Generate isotropic kernels\n",
    "# # isotropic_kernels = ngssf.data.benchmark_variances()\n",
    "# isotropic_kernels = torch.tensor([1e-11, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2])\n",
    "# isotropic_kernels = torch.eye(anisotropic_paper.shape[-1])[None] * isotropic_kernels[:, None, None]\n",
    "# isotropic_kernels = isotropic_kernels.to(device)\n",
    "\n",
    "# # Generate anisotropic kernels\n",
    "# anisotropic_small = FieldDataGenerator._rand_covariance_matrices_with_logrand_eigenvalues(\n",
    "#     torch.tensor([-9.0, -4.8], device=device), torch.tensor([-6.0, -4.0], device=device), \n",
    "#     2, 100, \n",
    "# device=device)\n",
    "# anisotropic_medium = FieldDataGenerator._rand_covariance_matrices_with_logrand_eigenvalues(\n",
    "#     torch.tensor([-8.0, -4.0], device=device), torch.tensor([-5.0, -3.0], device=device), \n",
    "#     2, 100,\n",
    "# device=device)\n",
    "# anisotropic_large = FieldDataGenerator._rand_covariance_matrices_with_logrand_eigenvalues(\n",
    "#     torch.tensor([-7.0, -3.0], device=device), torch.tensor([-4.0, -2.2], device=device), \n",
    "#     2, 100,\n",
    "# device=device)\n",
    "\n",
    "# Generate isotropic kernels from the paper\n",
    "isotropic_kernels_list_paper = [1e-12, 0.0001, 0.001, 0.01, 0.1]\n",
    "isotropic_kernels_paper = torch.tensor(isotropic_kernels_list_paper)\n",
    "isotropic_kernels_paper = torch.eye(anisotropic_paper.shape[-1])[None] * isotropic_kernels_paper[:, None, None]\n",
    "isotropic_kernels_paper = isotropic_kernels_paper.to(device, dtype=torch.float32)\n",
    "\n",
    "# Generate video kernels\n",
    "video_kernels = ngssf.util._interpolate_2d_covariance_matrices(\n",
    "    np.array([0, 25, 100, 150, 200, 230, 280, 310, 360, 438]),\n",
    "    np.array([\n",
    "        [0, -7, -7], [0, -4, -4], [0, -1, -1], [0, -4, -1], [0.125, -4, -1], \n",
    "        [0.125, -4, -2], [0.25, -4, -2], [0.25, -4, -3], [0.5, -4, -3], [1, -7, -7],\n",
    "    ])\n",
    ")\n",
    "\n",
    "# # 3D video kernels\n",
    "# times = np.array([0, 180, 300, 420, 540])\n",
    "# logvars = np.array([[-7, -7, -7], [-3, -3, -3], [-3, -7, -3], [-7, -3, -7], [-7, -7, -7]])\n",
    "# frames = np.arange(times.max() + 1)\n",
    "# vis_kernels_3d = np.zeros((len(frames), 3, 3))\n",
    "# vis_kernels_3d[:, 0, 0] = 10 ** np.interp(frames, times, logvars[:, 0])\n",
    "# vis_kernels_3d[:, 1, 1] = 10 ** np.interp(frames, times, logvars[:, 1])\n",
    "# vis_kernels_3d[:, 2, 2] = 10 ** np.interp(frames, times, logvars[:, 2])\n",
    "# vis_kernels_3d = torch.tensor(vis_kernels_3d, device=device, dtype=torch.float32)\n",
    "\n",
    "# # 3D Isotropic kernels\n",
    "# isotropic_kernels_3d = torch.tensor([1e-12, 0.0001, 0.001, 0.01, 0.1], dtype=torch.float32, device=\"cuda\")\n",
    "# isotropic_kernels_3d = torch.eye(3, device=\"cuda\")[None] * isotropic_kernels_3d[:, None, None]\n",
    "\n",
    "# # Save the kernels\n",
    "# testing_kernels = {\n",
    "#     # \"isotropic\": isotropic_kernels.to(\"cpu\")\n",
    "#     # \"original_only\": isotropic_kernels_paper[0][None].to(\"cpu\"),\n",
    "# }\n",
    "# for i, kernel in enumerate(isotropic_kernels_paper):\n",
    "#     testing_kernels[f\"isotropic_paper_{isotropic_kernels_list_paper[i]}\"] = kernel[None].to(\"cpu\", dtype=torch.float32)\n",
    "    \n",
    "# testing_kernels |= {\n",
    "#     # \"anisotropic_small\": anisotropic_small.to(\"cpu\"),\n",
    "#     # \"anisotropic_medium\": anisotropic_medium.to(\"cpu\"),\n",
    "#     # \"anisotropic_large\": anisotropic_large.to(\"cpu\"),\n",
    "#     \"anisotropic_paper\": anisotropic_paper.to(\"cpu\", dtype=torch.float32),\n",
    "#     # \"video\": cov_mats.to(\"cpu\")\n",
    "# }\n",
    "# torch.save({\"video\": vis_kernels_3d}, ngssf.data._data_dir / \"covariance_matrices\" / \"3d_vis.pt\")\n",
    "# torch.save({\"isotropic\": isotropic_kernels_3d}, ngssf.data._data_dir / \"covariance_matrices\" / \"3d_isotropic.pt\")\n",
    "# torch.save(testing_kernels, ngssf.data._data_dir / \"covariance_matrices\" / \"3d_paper.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "/opt/conda/lib/python3.10/site-packages/torchmetrics/functional/image/lpips.py:332: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(model_path, map_location=\"cpu\"), strict=False)\n",
      "[rank: 0] Checkpoint path tmp does not exist.\n"
     ]
    }
   ],
   "source": [
    "# experiments_root_path = Path(\"/Code/viscomp_reconstruction/Data/Experiments/ngssf/sdf_experiments\")\n",
    "experiments_root_path = Path(\"/Code/viscomp_reconstruction/Data/Experiments/ngssf/family_experiments\")\n",
    "\n",
    "# experiment_relative_path1 = \"background_anisotropic_gaussian/background-ngssf-gaussian-alien\"\n",
    "# experiment_relative_path1 = \"sdf-ours-gaussian-dragon\"\n",
    "experiment_relative_path1 = \"ngssf-1mc-nolipschitz-gaussian-curiouscat\"\n",
    "experiment_path1 = experiments_root_path / experiment_relative_path1\n",
    "cfg1 = get_cfg_from_experiment(\n",
    "    experiment_path=experiment_path1,\n",
    "    overrides=[\n",
    "        f\"paths.output_dir={str(experiment_path1)}\",\n",
    "        \"tensorboard=null\",\n",
    "        \"ckpt_path=tmp\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "# cfg1.dataset.field.source_field.voxel_resolution=256\n",
    "# cfg1.dataset.field.source_field.input_coord_cache = 0\n",
    "# cfg1.dataset.field.source_field.voxel_resolution = 1024\n",
    "trainer = Trainer(cfg1).to(device)\n",
    "_ = trainer.eval()\n",
    "\n",
    "# experiment_relative_path2 = \"ngssf_pretrained/ngssf-orig-picture-apples\"\n",
    "# experiment_path2 = experiments_root_path / experiment_relative_path2\n",
    "# cfg2 = get_cfg_from_experiment(\n",
    "#     experiment_path=experiment_path2,\n",
    "#     overrides=[\n",
    "#         f\"paths.output_dir={str(experiment_path1)}\",\n",
    "#         \"tensorboard=null\",\n",
    "#         \"ckpt_path=null\",\n",
    "#     ],\n",
    "#     device=device\n",
    "# )\n",
    "# trainer2 = Trainer(cfg2).to(device)\n",
    "# trainer2.load_state_dict_from_path(str(experiment_path2), load_training_state=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "could not get a list of mounted file-systems\n"
     ]
    }
   ],
   "source": [
    "import os, uuid, tempfile, numpy as np, cv2, trimesh, open3d as o3d\n",
    "import blendertoolbox as bt\n",
    "import os, math, bpy\n",
    "\n",
    "def _interpolate_3d_covariance_matrices(times, angles_xyz_and_logvars):\n",
    "    \"\"\"\n",
    "    Interpolate per-frame 3D covariance matrices from (angles, log-variances).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    times : 1D array-like of shape (K,)\n",
    "        Keyframe time indices (must be non-decreasing for np.interp).\n",
    "    angles_xyz_and_logvars : array-like of shape (K, 6)\n",
    "        Columns: [ax, ay, az, lvx, lvy, lvz]\n",
    "        - ax, ay, az are rotations about X/Y/Z respectively, given on [0,1) and\n",
    "          internally scaled to radians via angle * 2π.\n",
    "        - lvx, lvy, lvz are base-10 log-variances along the principal axes.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor of shape (T, 3, 3) on CUDA\n",
    "        Full covariance per frame t = 0..max(times):\n",
    "            Σ_t = R_t @ diag(10**lvx_t, 10**lvy_t, 10**lvz_t) @ R_t^T\n",
    "        Rotation convention: R = Rz(az) @ Ry(ay) @ Rx(ax)  (yaw-pitch-roll).\n",
    "    \"\"\"\n",
    "    times = np.asarray(times)\n",
    "    frames = np.arange(int(times.max()) + 1)\n",
    "\n",
    "    ax = np.interp(frames, times, angles_xyz_and_logvars[:, 0])\n",
    "    ay = np.interp(frames, times, angles_xyz_and_logvars[:, 1])\n",
    "    az = np.interp(frames, times, angles_xyz_and_logvars[:, 2])\n",
    "    lvx = np.interp(frames, times, angles_xyz_and_logvars[:, 3])\n",
    "    lvy = np.interp(frames, times, angles_xyz_and_logvars[:, 4])\n",
    "    lvz = np.interp(frames, times, angles_xyz_and_logvars[:, 5])\n",
    "\n",
    "    # Scale normalized angles to radians\n",
    "    ax = ax * (2 * np.pi)\n",
    "    ay = ay * (2 * np.pi)\n",
    "    az = az * (2 * np.pi)\n",
    "\n",
    "    cx, sx = np.cos(ax), np.sin(ax)\n",
    "    cy, sy = np.cos(ay), np.sin(ay)\n",
    "    cz, sz = np.cos(az), np.sin(az)\n",
    "\n",
    "    # R = Rz(az) @ Ry(ay) @ Rx(ax) ; explicit batched elements\n",
    "    r00 = cz * cy\n",
    "    r01 = cz * sy * sx - sz * cx\n",
    "    r02 = cz * sy * cx + sz * sx\n",
    "\n",
    "    r10 = sz * cy\n",
    "    r11 = sz * sy * sx + cz * cx\n",
    "    r12 = sz * sy * cx - cz * sx\n",
    "\n",
    "    r20 = -sy\n",
    "    r21 = cy * sx\n",
    "    r22 = cy * cx\n",
    "\n",
    "    rot_mats = np.stack(\n",
    "        [\n",
    "            np.stack([r00, r01, r02], axis=-1),\n",
    "            np.stack([r10, r11, r12], axis=-1),\n",
    "            np.stack([r20, r21, r22], axis=-1),\n",
    "        ],\n",
    "        axis=-2,  # shape (T, 3, 3)\n",
    "    )\n",
    "\n",
    "    var_mats = np.zeros((len(frames), 3, 3), dtype=np.float64)\n",
    "    var_mats[:, 0, 0] = 10.0 ** lvx\n",
    "    var_mats[:, 1, 1] = 10.0 ** lvy\n",
    "    var_mats[:, 2, 2] = 10.0 ** lvz\n",
    "\n",
    "    covs = rot_mats @ var_mats @ np.swapaxes(rot_mats, 1, 2)\n",
    "    return torch.as_tensor(covs, dtype=torch.float32, device=\"cuda\")\n",
    "\n",
    "def get_gt_pred_with_insets(batch, outputs):\n",
    "    gt, pred = batch[\"rgb\"], outputs[\"rgb\"]  # HxWx3, [-1,1], torch\n",
    "\n",
    "    def to_bgr_u8_and_01(x):\n",
    "        x01 = (x + 1.0) / 2.0\n",
    "        bgr = (x01.clamp(0,1) * 255).round().to(torch.uint8).cpu().numpy()[..., ::-1]\n",
    "        return bgr, x01.clamp(0,1)\n",
    "\n",
    "    gt_bgr, gt01   = to_bgr_u8_and_01(gt)\n",
    "    pred_bgr, pr01 = to_bgr_u8_and_01(pred)\n",
    "\n",
    "    H, W = pred_bgr.shape[:2]\n",
    "    m = max(2, int(min(H, W) * 0.02))      # margin\n",
    "    b = max(1, int(min(H, W) * 0.004))     # border\n",
    "    side = min(int((H - 3*m)//2 - 2*b), int(W * 0.30))\n",
    "    ih = side + 2*b                        # inset height with border\n",
    "\n",
    "    def place(img, ins, y):\n",
    "        ins = cv2.resize(ins, (side, side), interpolation=cv2.INTER_AREA)\n",
    "        ins = cv2.copyMakeBorder(ins, b, b, b, b, cv2.BORDER_CONSTANT, value=(255,255,255))\n",
    "        x = W - ins.shape[1] - m\n",
    "        img[y:y+ins.shape[0], x:x+ins.shape[1]] = ins\n",
    "\n",
    "    # --- error inset (×4 to dramatize), top-right ---\n",
    "    err = ((pr01 - gt01).abs().mean(-1) * 4.0).clamp(0,1).cpu().numpy()\n",
    "    err_u8  = (err * 255).round().astype(np.uint8)\n",
    "    cmap = cv2.COLORMAP_MAGMA if hasattr(cv2, \"COLORMAP_MAGMA\") else cv2.COLORMAP_INFERNO\n",
    "    err_vis = cv2.applyColorMap(err_u8, cmap)\n",
    "\n",
    "    # --- spectrum inset helper (expects CHW, returns [-1,1]) ---\n",
    "    def spectrum_vis(x_rgb_m1p1):\n",
    "        s = _spectrum(x_rgb_m1p1.permute(2,0,1).cpu())\n",
    "        if isinstance(s, torch.Tensor): s = s.detach().cpu()\n",
    "        s = np.asarray(s.squeeze())                         # HxW in [-1,1]\n",
    "        u8 = np.clip((s + 1.0) * 127.5, 0, 255).round().astype(np.uint8)\n",
    "        return cv2.cvtColor(u8, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    spec_pred = spectrum_vis(pred)\n",
    "    spec_gt   = spectrum_vis(gt)\n",
    "\n",
    "    # --- compose and save ---\n",
    "    out_pred = pred_bgr.copy()\n",
    "    place(out_pred, err_vis, m)                 # top-right\n",
    "    place(out_pred, spec_pred, H - ih - m)      # bottom-right\n",
    "    out_gt = gt_bgr.copy()\n",
    "    place(out_gt, spec_gt, H - ih - m)          # bottom-right\n",
    "\n",
    "    return out_gt, out_pred\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def render_ply(ply_filename, outputPath, is_lucy=False, use_vert_color=False,\n",
    "               imgRes_x=1024, imgRes_y=1024, samples_if_textured=100, samples_if_vcol=20,\n",
    "               exposure=1.5, use_GPU=True, blend_name='test.blend'):\n",
    "    # Initialize blender\n",
    "    numSamples = samples_if_vcol if use_vert_color else samples_if_textured\n",
    "    bt.blenderInit(imgRes_x, imgRes_y, numSamples, exposure, use_GPU)\n",
    "\n",
    "    # Load the blend file\n",
    "    cwd = os.getcwd()\n",
    "    blend_file = os.path.join(cwd, blend_name)\n",
    "    with bpy.data.libraries.load(blend_file, link=False) as (data_from, data_to):\n",
    "        data_to.objects = data_from.objects\n",
    "\n",
    "    # Link all objects\n",
    "    for obj in data_to.objects:\n",
    "        if obj is not None:\n",
    "            bpy.context.scene.collection.objects.link(obj)\n",
    "\n",
    "    # Reference GT object (for transform/material)\n",
    "    gt_obj = bpy.data.objects.get('gt')\n",
    "    if gt_obj is None:\n",
    "        raise SystemExit('GT object not found in the blend file.')\n",
    "\n",
    "    # Store transform + material then hide GT\n",
    "    location = tuple(gt_obj.location)\n",
    "    rotation = tuple(math.degrees(r) for r in gt_obj.rotation_euler)\n",
    "    scale    = tuple(gt_obj.scale)\n",
    "    gt_mat   = gt_obj.active_material\n",
    "    gt_obj.hide_viewport = True\n",
    "    gt_obj.hide_render   = True\n",
    "\n",
    "    # Import new mesh at GT transform\n",
    "    ply_path = os.path.join(cwd, ply_filename)\n",
    "    if not os.path.exists(ply_path):\n",
    "        raise SystemExit(f'PLY file not found: {ply_path}')\n",
    "    new_mesh = bt.readMesh(ply_path, location, rotation, scale)\n",
    "\n",
    "    # Shading\n",
    "    if use_vert_color:\n",
    "        # emission from vertex colors (layer name \"Col\" on PLY import)\n",
    "        mat = bpy.data.materials.new(name=\"VertexColorFlatMaterial\")\n",
    "        mat.use_nodes = True\n",
    "        nodes = mat.node_tree.nodes; nodes.clear()\n",
    "        vc_node  = nodes.new(type=\"ShaderNodeVertexColor\"); vc_node.layer_name = \"Col\"\n",
    "        emission = nodes.new(type=\"ShaderNodeEmission\")\n",
    "        output   = nodes.new(type=\"ShaderNodeOutputMaterial\")\n",
    "        mat.node_tree.links.new(vc_node.outputs['Color'], emission.inputs['Color'])\n",
    "        mat.node_tree.links.new(emission.outputs['Emission'], output.inputs['Surface'])\n",
    "\n",
    "        if new_mesh.data.materials:\n",
    "            new_mesh.data.materials[0] = mat\n",
    "        else:\n",
    "            new_mesh.data.materials.append(mat)\n",
    "\n",
    "        # Remove ground so it doesn’t tint the inset render\n",
    "        plane = bpy.data.objects.get('Plane')\n",
    "        if plane:\n",
    "            bpy.ops.object.select_all(action='DESELECT')\n",
    "            plane.select_set(True)\n",
    "            bpy.context.view_layer.objects.active = plane\n",
    "            bpy.ops.object.delete()\n",
    "    else:\n",
    "        bpy.ops.object.shade_smooth()\n",
    "        if gt_mat:\n",
    "            new_mesh.active_material = gt_mat\n",
    "\n",
    "    cam = bpy.data.objects.get('Camera')\n",
    "    if cam is None:\n",
    "        raise SystemExit('Camera not found in the blend file.')\n",
    "\n",
    "    if is_lucy:\n",
    "        new_mesh.rotation_euler[0] = 0\n",
    "\n",
    "    # Save updated blend (optional)\n",
    "    # bpy.ops.wm.save_mainfile(filepath=os.path.join(cwd, 'test_updated.blend'))\n",
    "\n",
    "    bt.renderImage(outputPath, cam)\n",
    "    return outputPath\n",
    "\n",
    "\n",
    "def _place_top_right_inset(base_bgr, inset_bgr, margin_frac=0.02, border_frac=0.004):\n",
    "    H, W = base_bgr.shape[:2]\n",
    "    m = max(2, int(min(H, W) * margin_frac))\n",
    "    b = max(1, int(min(H, W) * border_frac))\n",
    "    side = min(int((H - 3*m)//2 - 2*b), int(W * 0.30))\n",
    "    ins = cv2.resize(inset_bgr, (side, side), interpolation=cv2.INTER_AREA)\n",
    "    ins = cv2.copyMakeBorder(ins, b, b, b, b, cv2.BORDER_CONSTANT, value=(255,255,255))\n",
    "    out = base_bgr.copy()\n",
    "    x = W - ins.shape[1] - m; y = m\n",
    "    out[y:y+ins.shape[0], x:x+ins.shape[1]] = ins\n",
    "    return out\n",
    "\n",
    "def get_gt_pred_with_insets_sdf(gt_tm: trimesh.Trimesh,\n",
    "                                pred_tm: trimesh.Trimesh,\n",
    "                                resolution=1024,\n",
    "                                err_mult=1.0,\n",
    "                                clip_percentile=95,\n",
    "                                is_lucy=False,\n",
    "                                blend_name='test.blend'):\n",
    "    \"\"\"\n",
    "    Returns: (gt_bgr, pred_with_error_inset_bgr), both at `resolution`×`resolution`.\n",
    "    Renders are produced by Blender; error uses Open3D SDF (pred→gt), colored with MAGMA.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- 1) SDF error with Open3D (pred→gt) ---\n",
    "    def _tm_to_o3d(m: trimesh.Trimesh):\n",
    "        o = o3d.geometry.TriangleMesh()\n",
    "        o.vertices = o3d.utility.Vector3dVector(m.vertices.astype(np.float64))\n",
    "        o.triangles = o3d.utility.Vector3iVector(m.faces.astype(np.int32))\n",
    "        return o\n",
    "\n",
    "    scene = o3d.t.geometry.RaycastingScene()\n",
    "    _ = scene.add_triangles(o3d.t.geometry.TriangleMesh.from_legacy(_tm_to_o3d(gt_tm)))\n",
    "    pred_pts_legacy = o3d.geometry.PointCloud(o3d.utility.Vector3dVector(pred_tm.vertices.astype(np.float64)))\n",
    "    pred_pts_t = o3d.t.geometry.PointCloud.from_legacy(pred_pts_legacy)\n",
    "    sdf = scene.compute_signed_distance(pred_pts_t.point.positions).numpy()  # (N,)\n",
    "\n",
    "    err = np.abs(sdf) * float(err_mult)\n",
    "    if clip_percentile is not None:\n",
    "        mx = np.percentile(err, clip_percentile); mx = mx if mx > 1e-12 else 1.0\n",
    "        err01 = np.clip(err / mx, 0.0, 1.0)\n",
    "    else:\n",
    "        err01 = np.clip(err, 0.0, 1.0)\n",
    "    err_u8 = (err01 * 255).astype(np.uint8)\n",
    "\n",
    "    # Color with MAGMA\n",
    "    cmap = cv2.COLORMAP_MAGMA if hasattr(cv2, \"COLORMAP_MAGMA\") else cv2.COLORMAP_INFERNO\n",
    "    col_bgr = cv2.applyColorMap(err_u8[:, None], cmap)[:, 0, :]     # Nx3\n",
    "    col_rgb = col_bgr[:, ::-1]\n",
    "    col_rgba = np.concatenate([col_rgb, 255*np.ones((col_rgb.shape[0], 1), dtype=np.uint8)], axis=1)\n",
    "\n",
    "    pred_col = pred_tm.copy()\n",
    "    pred_col.visual.vertex_colors = col_rgba  # per-vertex RGBA (uint8)\n",
    "\n",
    "    # --- 2) Export temp PLYs for Blender ---\n",
    "    tmpdir = tempfile.mkdtemp(prefix=\"meshviz_\")\n",
    "    fn_gt   = os.path.join(tmpdir, f\"gt_{uuid.uuid4().hex}.ply\")\n",
    "    fn_pred = os.path.join(tmpdir, f\"pred_{uuid.uuid4().hex}.ply\")  # colored\n",
    "\n",
    "    gt_tm.export(fn_gt)\n",
    "    pred_col.export(fn_pred)  # carries vertex colors for the error render\n",
    "\n",
    "    # --- 3) Blender renders (plain GT, plain Pred, error Pred via vertex colors) ---\n",
    "    fn_img_gt   = os.path.join(tmpdir, f\"gt_{uuid.uuid4().hex}.png\")\n",
    "    fn_img_pred = os.path.join(tmpdir, f\"pred_{uuid.uuid4().hex}.png\")\n",
    "    fn_img_err  = os.path.join(tmpdir, f\"pred_err_{uuid.uuid4().hex}.png\")\n",
    "\n",
    "    # GT: textured/smooth (no vertex color)\n",
    "    render_ply(fn_gt,   fn_img_gt,   is_lucy=is_lucy, use_vert_color=False,\n",
    "               imgRes_x=resolution, imgRes_y=resolution, blend_name=blend_name)\n",
    "\n",
    "    # PRED base: use the same PLY but ignore vertex colors (material shades it)\n",
    "    render_ply(fn_pred, fn_img_pred, is_lucy=is_lucy, use_vert_color=False,\n",
    "               imgRes_x=resolution, imgRes_y=resolution, blend_name=blend_name)\n",
    "\n",
    "    # PRED error: emission from vertex colors\n",
    "    render_ply(fn_pred, fn_img_err,  is_lucy=is_lucy, use_vert_color=True,\n",
    "               imgRes_x=resolution, imgRes_y=resolution, blend_name=blend_name)\n",
    "\n",
    "    # --- 4) Compose inset and return frames (BGR) ---\n",
    "    gt_bgr   = cv2.imread(fn_img_gt,  cv2.IMREAD_COLOR)\n",
    "    pred_bgr = cv2.imread(fn_img_pred, cv2.IMREAD_COLOR)\n",
    "    err_bgr  = cv2.imread(fn_img_err,  cv2.IMREAD_COLOR)\n",
    "\n",
    "    pred_with_inset = _place_top_right_inset(pred_bgr, err_bgr)\n",
    "    return gt_bgr, pred_with_inset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anisotropic_sdf = _interpolate_3d_covariance_matrices(\n",
    "#     [0, 15, 65, 100, 135, 155, 190, 210, 245, 299],\n",
    "#     np.array([\n",
    "#         [0, 0, 0, -7, -7, -7], \n",
    "#         [0, 0, 0, -4, -4, -4], \n",
    "#         [0, 0, 0, -3, -3, -3], \n",
    "#         [0, 0, 0, -4, -1, -4], \n",
    "#         [0.125, 0.0, 0.0, -4, -1, -4], \n",
    "#         [0.125, 0.0, 0.0, -4, -3, -4], \n",
    "#         [0.25, 0.25, 0.0, -4, -3, -3], \n",
    "#         [0.25, 0.25, 0.0, -4, -3, -3], \n",
    "#         [0.5, 0.25, 0.25, -4, -3, -3], \n",
    "#         [1, 0.0, 0.0, -7, -7, -7]\n",
    "#     ])\n",
    "# )\n",
    "\n",
    "# for kernel in [\"gaussian\", \"uniform_ellipsoid\", \"lanczos\"]:\n",
    "#     trainer.model.encoder.smoothing_mode = kernel\n",
    "#     trainer.dataset.field._sampling_mode = kernel\n",
    "    \n",
    "#     # gt_recorder = VideoEncoderThread(\n",
    "#     #     f\"output/gt_{kernel}.mp4\",\n",
    "#     #     fps=30,\n",
    "#     #     width=resolution,\n",
    "#     #     height=resolution,\n",
    "#     #     options={\n",
    "#     #         \"preset\": \"slow\",       # Slower encoding yields better compression efficiency\n",
    "#     #         \"crf\": \"23\",            # Near visually lossless quality (experiment with higher values if acceptable)\n",
    "#     #         \"tune\": \"stillimage\",   # Optimizes compression for nearly static content\n",
    "#     #         \"sc_threshold\": \"0\",    # Disables scene cut detection to enforce a fixed GOP\n",
    "#     #     }\n",
    "#     # )\n",
    "#     # pred_recorder = VideoEncoderThread(\n",
    "#     #     f\"output/pred_{kernel}.mp4\",\n",
    "#     #     fps=30,\n",
    "#     #     width=resolution,\n",
    "#     #     height=resolution,\n",
    "#     #     options={\n",
    "#     #         \"preset\": \"slow\",       # Slower encoding yields better compression efficiency\n",
    "#     #         \"crf\": \"23\",            # Near visually lossless quality (experiment with higher values if acceptable)\n",
    "#     #         \"tune\": \"stillimage\",   # Optimizes compression for nearly static content\n",
    "#     #         \"sc_threshold\": \"0\",    # Disables scene cut detection to enforce a fixed GOP\n",
    "#     #     }\n",
    "#     # )\n",
    "#     # gt_recorder.start()\n",
    "#     # pred_recorder.start()\n",
    "    \n",
    "#     for index in tqdm(range(len(anisotropic_sdf))):\n",
    "#         index = 100\n",
    "#         scale = anisotropic_sdf[[index]]\n",
    "    \n",
    "#         # Get the test data\n",
    "#         batch = trainer.dataset.get_test_data(\n",
    "#             scale,\n",
    "#             cfg1.trainer.testing_resolution,\n",
    "#             batch_size=trainer.cfg.trainer.testing_batch_size,\n",
    "#         )\n",
    "    \n",
    "#         # Run the model\n",
    "#         # Perform the testing step\n",
    "#         with torch.no_grad():\n",
    "#             outputs = trainer.run_batched(batch, trainer.cfg.trainer.testing_batch_size)\n",
    "#             # Calculate the metrics\n",
    "#             loss, metrics = trainer.model.calculate_loss_and_log(\n",
    "#                 batch, outputs, trainer, log_mode=\"test_with_vis\")\n",
    "    \n",
    "#         out_gt, out_pred = get_gt_pred_with_insets_sdf(\n",
    "#             metrics['mesh']['gt'], metrics['mesh']['pred'])\n",
    "\n",
    "#         break\n",
    "#         # gt_recorder.enqueue_frame(out_gt)\n",
    "#         # pred_recorder.enqueue_frame(out_pred)\n",
    "        \n",
    "#     break\n",
    "#     # gt_recorder.stop()\n",
    "#     # pred_recorder.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d5966d784924448b571578ad844d022",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1009 23:51:17.944000 139705020405568 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] xindex is not in var_ranges, defaulting to unknown range.\n",
      "W1009 23:51:18.662000 139705020405568 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] xindex is not in var_ranges, defaulting to unknown range.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f656d00497949fa87d5dfcb8019aa3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1010 00:41:10.170000 139705020405568 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] xindex is not in var_ranges, defaulting to unknown range.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1928eca3ed67418899c0e254a1e7a625",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1010 01:34:40.139000 139705020405568 torch/fx/experimental/symbolic_shapes.py:4449] [2/0] xindex is not in var_ranges, defaulting to unknown range.\n",
      "W1010 01:34:41.659000 139705020405568 torch/fx/experimental/symbolic_shapes.py:4449] [2/1] xindex is not in var_ranges, defaulting to unknown range.\n"
     ]
    }
   ],
   "source": [
    "resolution = 2048\n",
    "kernels_image = ngssf.util._interpolate_2d_covariance_matrices(\n",
    "    np.array([0, 15, 55, 85, 115, 130, 160, 175, 205, 249]),\n",
    "    np.array([\n",
    "        [0, -7, -7], [0, -4, -4], [0, -1, -1], [0, -4, -1], [0.125, -4, -1], \n",
    "        [0.125, -3, -2], [0.25, -3, -2], [0.25, -4, -3], [0.5, -4, -3], [1, -7, -7],\n",
    "    ])\n",
    ").cuda()\n",
    "\n",
    "\n",
    "for kernel in [\"gaussian\", \"uniform_ellipsoid\", \"lanczos\"]:\n",
    "    trainer.model.encoder.smoothing_mode = kernel\n",
    "    trainer.dataset.field._sampling_mode = kernel\n",
    "    \n",
    "    gt_recorder = VideoEncoderThread(\n",
    "        f\"output/gt_{kernel}.mp4\",\n",
    "        fps=25,\n",
    "        width=resolution,\n",
    "        height=resolution,\n",
    "        options={\n",
    "            \"preset\": \"slow\",       # Slower encoding yields better compression efficiency\n",
    "            \"crf\": \"23\",            # Near visually lossless quality (experiment with higher values if acceptable)\n",
    "            \"tune\": \"stillimage\",   # Optimizes compression for nearly static content\n",
    "            \"sc_threshold\": \"0\",    # Disables scene cut detection to enforce a fixed GOP\n",
    "        }\n",
    "    )\n",
    "    pred_recorder = VideoEncoderThread(\n",
    "        f\"output/pred_{kernel}.mp4\",\n",
    "        fps=25,\n",
    "        width=resolution,\n",
    "        height=resolution,\n",
    "        options={\n",
    "            \"preset\": \"slow\",       # Slower encoding yields better compression efficiency\n",
    "            \"crf\": \"23\",            # Near visually lossless quality (experiment with higher values if acceptable)\n",
    "            \"tune\": \"stillimage\",   # Optimizes compression for nearly static content\n",
    "            \"sc_threshold\": \"0\",    # Disables scene cut detection to enforce a fixed GOP\n",
    "        }\n",
    "    )\n",
    "    kernel_recorder = VideoEncoderThread(\n",
    "        f\"output/kernel_{kernel}.mp4\",\n",
    "        fps=25,\n",
    "        width=1024,\n",
    "        height=1024,\n",
    "        options={\n",
    "            \"preset\": \"slow\",       # Slower encoding yields better compression efficiency\n",
    "            \"crf\": \"23\",            # Near visually lossless quality (experiment with higher values if acceptable)\n",
    "            \"tune\": \"stillimage\",   # Optimizes compression for nearly static content\n",
    "            \"sc_threshold\": \"0\",    # Disables scene cut detection to enforce a fixed GOP\n",
    "        }\n",
    "    )\n",
    "    gt_recorder.start()\n",
    "    pred_recorder.start()\n",
    "    kernel_recorder.start()\n",
    "    \n",
    "    for index in tqdm(range(len(kernels_image))):\n",
    "        scale = kernels_image[[index]]\n",
    "    \n",
    "        # Get the test data\n",
    "        batch = trainer.dataset.get_test_data(\n",
    "            scale,\n",
    "            resolution,\n",
    "            batch_size=trainer.cfg.trainer.testing_batch_size,\n",
    "        )\n",
    "    \n",
    "        # Run the model\n",
    "        # Perform the testing step\n",
    "        with torch.no_grad():\n",
    "            outputs = trainer.run_batched(batch, trainer.cfg.trainer.testing_batch_size)\n",
    "            # Calculate the metrics\n",
    "            # loss, metrics = trainer.model.calculate_loss_and_log(\n",
    "            #     batch, outputs, trainer, log_mode=\"test_with_vis\")\n",
    "    \n",
    "        out_gt, out_pred = get_gt_pred_with_insets(\n",
    "            batch, outputs)\n",
    "        gt_recorder.enqueue_frame(out_gt)\n",
    "        pred_recorder.enqueue_frame(out_pred)\n",
    "    \n",
    "        kernel_vis = render_kernel(\n",
    "            kernel, scale, interval=(-0.5, 0.5),\n",
    "        )\n",
    "        kernel_recorder.enqueue_frame(kernel_vis)\n",
    "    \n",
    "    gt_recorder.stop()\n",
    "    pred_recorder.stop()\n",
    "    kernel_recorder.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = trainer1\n",
    "\n",
    "# trainer.eval()\n",
    "\n",
    "# # Select the kernel\n",
    "# index = 1\n",
    "# scale = anisotropic_paper[[index]]\n",
    "# # scale = torch.eye(3, device=device)[None] * 1e-6\n",
    "\n",
    "# print(scale)\n",
    "\n",
    "# # Get the test data\n",
    "# batch = trainer.dataset.get_test_data(\n",
    "#     scale,\n",
    "#     256,\n",
    "#     batch_size=trainer.cfg.trainer.testing_batch_size,\n",
    "# )\n",
    "\n",
    "# # Run the model\n",
    "# # Perform the testing step\n",
    "# with torch.no_grad():\n",
    "#     outputs = trainer.run_batched(batch, trainer.cfg.trainer.testing_batch_size)\n",
    "#     # Calculate the metrics\n",
    "#     loss, metrics = trainer.model.calculate_loss_and_log(\n",
    "#         batch, outputs, trainer, log_mode=\"test_with_vis\")\n",
    "    \n",
    "# if \"vis\" in metrics:\n",
    "#     vis = metrics.pop(\"vis\")\n",
    "#     fig = plt.figure(figsize=(12, 12))\n",
    "#     plt.imshow(vis[..., [2,1,0]])\n",
    "#     plt.axis(\"off\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
