{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facba287",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Sequence\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.patches as patches\n",
    "from omegaconf import OmegaConf\n",
    "import hydra\n",
    "from hydra import initialize_config_dir, compose\n",
    "import time\n",
    "import imageio\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import spnf\n",
    "from spnf.utils import set_seed, make_coord_grid, apply_to_tensors, to_py, interpolate_covariance_matrices_numpy\n",
    "from spnf.sample import (\n",
    "    rand_ortho, logrand, construct_covariance, sample_gaussian_delta, sample_ellipsoid_delta\n",
    ")\n",
    "from spnf.trainer import Trainer\n",
    "\n",
    "config_dir = Path(spnf.__file__).parent / \"configs\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def get_cfg(config_name, config_dir: Path, overrides: list):\n",
    "    with initialize_config_dir(version_base=None, config_dir=str(config_dir)):\n",
    "        cfg = compose(config_name=config_name, overrides=overrides)\n",
    "    return cfg\n",
    "\n",
    "def render_kernel(kind, cov_mat, interval=(-3.0, 3.0), res=1024, a=1.0, colormap=None):\n",
    "    device = cov_mat.device\n",
    "    lo, hi = float(interval[0]), float(interval[1])\n",
    "    x = torch.linspace(lo, hi, res, device=device)\n",
    "    grid = torch.stack(torch.meshgrid(x, x, indexing='xy'), dim=-1).reshape(-1, 2)  # (res*res, 2)\n",
    "\n",
    "    inv_cov = torch.linalg.inv(cov_mat)\n",
    "    v = grid @ inv_cov\n",
    "    q = (v * grid).sum(-1)  # quadratic form\n",
    "\n",
    "    if kind.lower() == 'gaussian':\n",
    "        ker = torch.exp(-0.5 * q)                                    # [0,1]\n",
    "        ker01 = ker.clamp(0, 1)\n",
    "    elif kind.lower() == 'uniform_ellipsoid':\n",
    "        ker = (q.sqrt() < 1).to(grid.dtype)                           # {0,1}\n",
    "        ker01 = ker\n",
    "    elif kind.lower() == 'lanczos':\n",
    "        t = q.sqrt()\n",
    "        # torch.sinc(x) = sin(pi*x)/(pi*x)\n",
    "        ker = torch.sinc(t) * torch.sinc(t / a)                       # can be [-1,1]\n",
    "        ker = ker / torch.max(ker.abs()) * 2.5\n",
    "        ker01 = ker.clamp(-1, 1).abs()\n",
    "        ker01 = ker01.clamp(0,1)\n",
    "    else:\n",
    "        raise ValueError(\"kind must be 'gaussian', 'uniform_ellipsoid', or 'lanczos'\")\n",
    "\n",
    "    img_u8 = (ker01.reshape(res, res) * 255).round().to(torch.uint8).cpu().numpy()\n",
    "\n",
    "    if colormap is not None:\n",
    "        vis = cv2.applyColorMap(img_u8, colormap)                     # 3ch BGR\n",
    "    else:\n",
    "        vis = cv2.cvtColor(img_u8, cv2.COLOR_GRAY2BGR)                # grayscale 3ch\n",
    "\n",
    "    return vis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6151715",
   "metadata": {},
   "source": [
    "## Training visualization for neural fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a04282",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_type = \"gaussian\" # Options: 'gaussian', 'uniform_ellipsoid', 'lanczos'\n",
    "lambda_decay_start = 500\n",
    "vis_steps = 800\n",
    "total_steps = 2000\n",
    "SRC_RES = 512\n",
    "FPS = 30\n",
    "cfg = get_cfg(\n",
    "    config_name=\"train\",\n",
    "    config_dir=config_dir,\n",
    "    overrides=[\n",
    "        f\"data.grid.resize={SRC_RES}\",\n",
    "        \"data.bounds=1.0\", \n",
    "        \"tensorboard=null\",\n",
    "        f\"trainer.steps={total_steps}\",\n",
    "        \"paths.output_dir=/home/myaldiz/Data/Experiments/spnf/${task_name}\",\n",
    "        f\"scheduler.lr_lambda.decay_start={lambda_decay_start}\",\n",
    "        \"trainer.compile_train_step=True\",\n",
    "    ],\n",
    ")\n",
    "video_settings = dict(\n",
    "    fps=FPS,\n",
    "    codec='libx265', \n",
    "    pixelformat='yuv420p',\n",
    "    ffmpeg_params = [\n",
    "        '-crf', '18', '-preset', 'slow', \n",
    "        '-tag:v', 'hvc1', '-tune', 'grain'\n",
    "    ]\n",
    ")\n",
    "dev_video_settings = video_settings | dict(\n",
    "    ffmpeg_params=[\n",
    "        '-crf', '20', \n",
    "        '-preset', 'fast',\n",
    "        '-tag:v', 'hvc1',\n",
    "        '-tune', 'grain',\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create output directories\n",
    "output_dir = Path(cfg.paths.output_dir)\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "checkpoint_dir = Path(cfg.trainer.checkpoint_dir)\n",
    "checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "video_output_dir = output_dir / \"visualizations\" \n",
    "video_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save the config to output directory\n",
    "OmegaConf.save(config=cfg, f=output_dir / \"config_multiscale.yaml\", resolve=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e002cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the trainer\n",
    "trainer = Trainer(cfg).to(device)\n",
    "\n",
    "# Get the gt image saved as a reference\n",
    "gt_signal = trainer.data.grid.to(device)\n",
    "gt_signal = gt_signal[0].add(1.0).div(2.0).mul(255).round().clamp(0, 255).permute(1,2,0).detach().cpu().numpy().astype(np.uint8)\n",
    "imageio.imwrite(\n",
    "    output_dir / \"visualizations\" / \"ground_truth.png\", \n",
    "    gt_signal\n",
    ")\n",
    "\n",
    "repeat_frames = FPS * 3\n",
    "times = np.array([0.0, 0.125, 0.25, 0.375, 0.5, 0.75, 0.85, 1.0])\n",
    "vis_covs, vis_eigvals, vis_eigvecs = interpolate_covariance_matrices_numpy(\n",
    "    np.round(times * (repeat_frames-1)).astype(np.int32),\n",
    "    np.array([\n",
    "        [0, -2, -7], [0.5, -4, -4], [0.0, -1, -1], [0.0, -3, -2], [0.5, -3, -2], \n",
    "        [0, -1, -1], [1.0, -4, -4], [1.0, -2, -7]\n",
    "    ])\n",
    ")\n",
    "vis_covs = torch.tensor(vis_covs, device=device, dtype=torch.float32)\n",
    "vis_eigvals = torch.tensor(vis_eigvals, device=device, dtype=torch.float32)\n",
    "vis_eigvecs = torch.tensor(vis_eigvecs, device=device, dtype=torch.float32)\n",
    "\n",
    "VIS_PATCH_RES = 64\n",
    "vis_patch_x, vis_patch_y = 190, 120\n",
    "\n",
    "loss_average = []\n",
    "predictions, kernel_images, train_images = [], [], []\n",
    "steps_history = []  # New: Track the actual step number for the X-axis\n",
    "\n",
    "current_step = 0\n",
    "frame_count = 0\n",
    "noise_steps = 5\n",
    "pbar = tqdm(total=vis_steps, desc=\"Multiscale Training\")\n",
    "\n",
    "while current_step < vis_steps:\n",
    "    # Determine stride based on how many frames we have generated\n",
    "    if current_step < 200:\n",
    "        stride = 1\n",
    "    else:\n",
    "        stride = 2\n",
    "        \n",
    "    # Ensure we don't exceed vis_steps\n",
    "    if current_step + stride > vis_steps:\n",
    "        stride = vis_steps - current_step\n",
    "        if stride == 0: break\n",
    "\n",
    "    # Fit for 'stride' steps\n",
    "    stats = trainer.fit(num_steps=stride, no_tqdm=True)\n",
    "    \n",
    "    # Update trackers\n",
    "    current_step += stride\n",
    "    \n",
    "    # Store Data\n",
    "    loss_average.append(np.mean(stats['mse_loss']))\n",
    "    steps_history.append(current_step) # Store actual X-axis location\n",
    "    \n",
    "    current_cov = vis_covs[(frame_count//noise_steps) % vis_covs.shape[0]]\n",
    "    current_eigvals = vis_eigvals[(frame_count//noise_steps) % vis_eigvals.shape[0]]\n",
    "    current_eigvecs = vis_eigvecs[(frame_count//noise_steps) % vis_eigvecs.shape[0]]\n",
    "    \n",
    "    # Generate visualization\n",
    "    output = trainer.generate_grid_data(\n",
    "        resolution=SRC_RES,\n",
    "        covariances=current_cov\n",
    "    )\n",
    "    predictions.append(output[\"filtered_signal\"].clamp(-1.0, 1.0).add(1.0).div(2.0).cpu().numpy())\n",
    "    kernel_images.append(\n",
    "        render_kernel(\n",
    "            kind=filter_type,\n",
    "            cov_mat=current_cov,\n",
    "            res=SRC_RES,\n",
    "        )\n",
    "    )\n",
    "    # Generate an example training image used for visualization.\n",
    "    train_images.append(\n",
    "        trainer.generate_data_train(\n",
    "            coords=output[\"coords\"],\n",
    "            eigenvalues=current_eigvals.unsqueeze(0).repeat(SRC_RES*SRC_RES, 1),\n",
    "            eigenvectors=current_eigvecs.unsqueeze(0).repeat(SRC_RES*SRC_RES, 1, 1),\n",
    "        )[\"gt_signal\"].clamp(-1.0, 1.0).add(1.0).div(2.0).cpu().numpy()\n",
    "    )\n",
    "    \n",
    "    frame_count += 1\n",
    "    pbar.update(stride)\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "# --- VIDEO GENERATION ---\n",
    "video_output = output_dir / \"visualizations\" / \"multiscale_train_vis.mov\"\n",
    "video_patch_output = output_dir / \"visualizations\" / \"multiscale_train_vis_patch.mov\"\n",
    "video_kernel_output = output_dir / \"visualizations\" / \"multiscale_train_vis_kernel.mov\"\n",
    "video_data_output = output_dir / \"visualizations\" / \"multiscale_train_vis_data.mov\"\n",
    "writer = imageio.get_writer(video_output, **video_settings)\n",
    "writer_patch = imageio.get_writer(video_patch_output, **video_settings)\n",
    "writer_kernel = imageio.get_writer(video_kernel_output, **video_settings)\n",
    "writer_data = imageio.get_writer(video_data_output, **video_settings)\n",
    "\n",
    "# Settings\n",
    "plt.style.use('default') \n",
    "max_loss = max(loss_average) * 1.1\n",
    "\n",
    "# 1. Geometry Calculation\n",
    "sample = predictions[0]\n",
    "if sample.ndim == 3 and sample.shape[0] in [1, 3]:\n",
    "    h, w = sample.shape[1], sample.shape[2]\n",
    "else:\n",
    "    h, w = sample.shape[0], sample.shape[1]\n",
    "\n",
    "fig_width = 12 \n",
    "img_height_in = fig_width * (h / w)\n",
    "loss_height_in = 4.0 \n",
    "total_height = img_height_in + loss_height_in\n",
    "\n",
    "# Calculate relative height ratios for manual positioning\n",
    "h_img_ratio = img_height_in / total_height\n",
    "h_loss_ratio = loss_height_in / total_height\n",
    "\n",
    "# repeat_ranges, repeat_times = [(30, 45), (60, 75), (230, 245)], 10\n",
    "# repeat_ranges = [i for r in repeat_ranges for i in range(r[0], r[1])]\n",
    "print(\"Rendering video frames...\")\n",
    "# Zip with steps_history to get the correct X-axis value\n",
    "for i, (loss, pred, step_num) in enumerate(tqdm(zip(loss_average, predictions, steps_history), total=len(predictions))):\n",
    "    \n",
    "    # Create figure without subplots/layouts to allow manual placement\n",
    "    fig = plt.figure(figsize=(fig_width, total_height), dpi=100)\n",
    "    \n",
    "    # Bottom Plot\n",
    "    margin_bottom = 0.08\n",
    "    h_loss_actual = h_loss_ratio - margin_bottom - 0.02\n",
    "    ax_loss = fig.add_axes([0.12, margin_bottom, 0.82, h_loss_actual])\n",
    "    \n",
    "    margin_top = 0.02\n",
    "    y_img_start = h_loss_ratio + 0.02\n",
    "    h_img_actual = h_img_ratio - margin_top - 0.02\n",
    "    ax_img = fig.add_axes([0.02, y_img_start, 0.96, h_img_actual])\n",
    "\n",
    "    # --- 1. Image Plot ---\n",
    "    if pred.ndim == 3 and pred.shape[0] in [1, 3]: \n",
    "        pred = np.transpose(pred, (1, 2, 0))\n",
    "    \n",
    "    ax_img.imshow(pred, aspect='auto') \n",
    "    \n",
    "    # Hide all ticks/spines for the image\n",
    "    ax_img.set_xticks([])\n",
    "    ax_img.set_yticks([])\n",
    "    for spine in ax_img.spines.values():\n",
    "        spine.set_visible(False) \n",
    "    \n",
    "    # --- 2. Loss Plot ---\n",
    "    # MODIFICATION: Use steps_history for X-axis data\n",
    "    current_steps_data = steps_history[:i+1]\n",
    "    current_loss_data = loss_average[:i+1]\n",
    "    \n",
    "    ax_loss.plot(current_steps_data, current_loss_data, color='tab:blue', linewidth=3)\n",
    "    \n",
    "    # MODIFICATION: Use step_num (the actual training step) for the scatter X-coordinate\n",
    "    ax_loss.scatter(step_num, loss, color='tab:red', s=100, zorder=5) \n",
    "    \n",
    "    # MODIFICATION: Set X limit to the total vis_steps (800) rather than frame count\n",
    "    ax_loss.set_xlim(0, vis_steps)\n",
    "    ax_loss.set_ylim(0, max_loss)\n",
    "    \n",
    "    ax_loss.set_ylabel('MSE Loss', fontsize=24, labelpad=15, fontweight='medium')\n",
    "    ax_loss.set_xlabel('Steps', fontsize=18) # Optional: Label x-axis\n",
    "    ax_loss.tick_params(axis='both', which='major', labelsize=16)\n",
    "    ax_loss.grid(True, linestyle='--', alpha=0.5, linewidth=1.5)\n",
    "\n",
    "    # --- 3. Save Frame ---\n",
    "    fig.canvas.draw()\n",
    "    frame = np.array(fig.canvas.buffer_rgba())\n",
    "    patch = pred[vis_patch_y:vis_patch_y+VIS_PATCH_RES, vis_patch_x:vis_patch_x+VIS_PATCH_RES]\n",
    "    \n",
    "    # Repeat the frames in specified range for slow down. \n",
    "    # if i in repeat_ranges:\n",
    "    #     for _ in range(repeat_times):\n",
    "    #         writer.append_data(frame)\n",
    "    #         writer_patch.append_data((patch * 255).astype(np.uint8))\n",
    "    #         writer_kernel.append_data(kernel_images[i])\n",
    "    #         writer_data.append_data((train_images[i] * 255).astype(np.uint8))\n",
    "    # else:\n",
    "    writer.append_data(frame)\n",
    "    writer_patch.append_data((patch * 255).astype(np.uint8))\n",
    "    writer_kernel.append_data(kernel_images[i])\n",
    "    writer_data.append_data((train_images[i] * 255).astype(np.uint8))\n",
    "    \n",
    "    plt.close(fig)\n",
    "\n",
    "writer.close()\n",
    "writer_patch.close()\n",
    "writer_kernel.close()\n",
    "writer_data.close()\n",
    "print(f\"Videos are saved: {video_output}, {video_patch_output}\")\n",
    "\n",
    "# Save last frame patch as PNG\n",
    "last_frame_output = output_dir / \"visualizations\" / \"multiscale_train_vis_patch.png\"\n",
    "patch = predictions[-1][vis_patch_y:vis_patch_y+VIS_PATCH_RES, vis_patch_x:vis_patch_x+VIS_PATCH_RES]\n",
    "imageio.imwrite(\n",
    "    last_frame_output, \n",
    "    (patch * 255).astype(np.uint8)\n",
    ")\n",
    "# Save the last traing image as PNG\n",
    "last_frame_data_output = output_dir / \"visualizations\" / \"multiscale_train_vis_data.png\"\n",
    "imageio.imwrite(\n",
    "    last_frame_data_output, \n",
    "    (train_images[-1] * 255).astype(np.uint8)\n",
    ")\n",
    "# Save the last predicted image as PNG\n",
    "last_frame_image_output = output_dir / \"visualizations\" / \"multiscale_train_vis_image.png\"\n",
    "final_pred = predictions[-1]\n",
    "if final_pred.ndim == 3 and final_pred.shape[0] in [1, 3]: \n",
    "    final_pred = np.transpose(final_pred, (1, 2, 0))\n",
    "imageio.imwrite(\n",
    "    last_frame_image_output, \n",
    "    (final_pred * 255).astype(np.uint8)\n",
    ")\n",
    "\n",
    "# Save the last kernel image as PNG\n",
    "last_frame_kernel_output = output_dir / \"visualizations\" / \"multiscale_train_vis_kernel.png\"\n",
    "imageio.imwrite(\n",
    "    last_frame_kernel_output, \n",
    "    kernel_images[-1]\n",
    ")\n",
    "\n",
    "print(f\"Last frame image saved: {last_frame_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00137275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train remaining steps\n",
    "trainer.fit(num_steps=total_steps - vis_steps)\n",
    "\n",
    "# Save the state dict\n",
    "state_dict = trainer.state_dict()\n",
    "checkpoint_path = checkpoint_dir / \"multiscale.pth\"\n",
    "torch.save(state_dict, str(checkpoint_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a09597",
   "metadata": {},
   "source": [
    "## Visualize changing covariance matrices vs predicted image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bbb45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = checkpoint_dir / \"multiscale.pth\"\n",
    "if not checkpoint_path.exists():\n",
    "    # Fit the model\n",
    "    trainer.fit()\n",
    "\n",
    "    # Save the state dict\n",
    "    state_dict = trainer.state_dict()\n",
    "    torch.save(state_dict, str(checkpoint_path))\n",
    "else:\n",
    "    # Load the state dict\n",
    "    trainer = Trainer(cfg).to(device)\n",
    "    state_dict = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "    trainer.load_state_dict(state_dict)\n",
    "    print(f\"Loaded model from {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122f68fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "FPS = 30\n",
    "SECONDS = 20\n",
    "FRAMES = FPS * SECONDS\n",
    "\n",
    "times = np.array([0.  , 0.05, 0.2, 0.35, 0.46, 0.52, 0.64, 0.7 , 0.82, 1.  ])\n",
    "vis_covs, _, _ = interpolate_covariance_matrices_numpy(\n",
    "    np.round(times * (FRAMES-1)).astype(np.int32),\n",
    "    np.array([\n",
    "        [0, -7, -7], [0, -4, -4], [0, -1, -1], [0, -4, -1.5], [0.125, -4, -1.5], \n",
    "        [0.125, -3, -2], [0.25, -3, -2], [0.25, -4, -3], [0.5, -4, -3], [1, -7, -7],\n",
    "    ])\n",
    ")\n",
    "vis_covs = torch.tensor(vis_covs, device=device, dtype=torch.float32)\n",
    "\n",
    "video_output = output_dir / \"visualizations\" / \"multiscale_filtering.mov\"\n",
    "kernel_output = output_dir / \"visualizations\" / \"multiscale_filtering_kernels.mov\"\n",
    "writer = imageio.get_writer(video_output, **dev_video_settings)\n",
    "writer_kernel = imageio.get_writer(kernel_output, **dev_video_settings)\n",
    "\n",
    "for i in tqdm(range(FRAMES)):\n",
    "    filtered_out = trainer.generate_grid_data(\n",
    "        resolution=SRC_RES,\n",
    "        covariances=vis_covs[i],\n",
    "        filter_type=filter_type,\n",
    "    )\n",
    "    filtered_out = filtered_out[\"filtered_signal\"].add(1.0).div(2.0).mul(255).round().clamp(0, 255).to(torch.uint8)\n",
    "    kernel_img = render_kernel(\n",
    "        kind=filter_type,\n",
    "        cov_mat=vis_covs[i],\n",
    "        res=SRC_RES,\n",
    "    )\n",
    "    \n",
    "    writer.append_data(filtered_out.cpu().numpy())\n",
    "    writer_kernel.append_data(kernel_img)\n",
    "    \n",
    "writer.close()\n",
    "writer_kernel.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cf62f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# ============ Visualization Script (Updated for Prefiltering) ============\n",
    "COLOR_BOX = '#D46CCD'\n",
    "COLOR_POINTS = '#4E71BE'\n",
    "SAMPLE_RES = 32\n",
    "FPS = 30\n",
    "SECONDS = 10\n",
    "FRAMES = FPS * SECONDS\n",
    "video_output = output_dir / \"visualizations\" / \"multiscale_aliasing_vis.mov\"\n",
    "\n",
    "# --- Keyframes (Fixed Timing) ---\n",
    "# Format: (Time_Percent, Center_X, Center_Y, Span)\n",
    "keyframes = [\n",
    "    (0.00, -0.5,  0.5, 0.7), # Start Top-Left\n",
    "    (0.40,  0.5,  0.5, 0.7), # Move Right\n",
    "    (0.50,  0.5, -0.4, 0.85), # Zoom Out & Down\n",
    "    (0.90, -0.5, -0.4, 0.85), # Move Left (Zoomed Out)\n",
    "    (1.00, -0.5,  0.5, 0.7), # Zoom In & Up\n",
    "]\n",
    "\n",
    "def get_trajectory_point_time(t_global, keyframes):\n",
    "    for k in range(len(keyframes) - 1):\n",
    "        t0, x0, y0, s0 = keyframes[k]\n",
    "        t1, x1, y1, s1 = keyframes[k+1]\n",
    "        if t0 <= t_global <= t1 + 1e-5:\n",
    "            segment_duration = t1 - t0\n",
    "            if segment_duration <= 1e-5: return x1, y1, s1\n",
    "            local_t = np.clip((t_global - t0) / segment_duration, 0, 1)\n",
    "            smooth_t = local_t * local_t * (3 - 2 * local_t) \n",
    "            current_x = x0 + (x1 - x0) * smooth_t\n",
    "            current_y = y0 + (y1 - y0) * smooth_t\n",
    "            current_s = s0 + (s1 - s0) * smooth_t\n",
    "            return current_x, current_y, current_s\n",
    "    return keyframes[-1][1:]\n",
    "\n",
    "def clamp_bounds(cx, cy, span, limit=1.0):\n",
    "    half_s = span / 2.0\n",
    "    if cx - half_s < -limit: cx = -limit + half_s\n",
    "    if cx + half_s > limit:  cx = limit - half_s\n",
    "    if cy - half_s < -limit: cy = -limit + half_s\n",
    "    if cy + half_s > limit:  cy = limit - half_s\n",
    "    return cx, cy\n",
    "\n",
    "# --- Setup Plot (Now 1 Row, 3 Columns) ---\n",
    "plt.style.use('seaborn-v0_8-white') \n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5.5), dpi=200, constrained_layout=True)\n",
    "fig.patch.set_facecolor('white')\n",
    "\n",
    "# 1. Context View (Ground Truth)\n",
    "full_field = trainer.data\n",
    "output = trainer.generate_grid_data(resolution=SRC_RES)\n",
    "reconstructed_data = output[\"filtered_signal\"].clamp(-1.0, 1.0).add(1.0).div(2.0).cpu().numpy()\n",
    "im_context = axs[0].imshow(reconstructed_data, extent=(-1, 1, 1, -1), cmap='gray', vmin=0, vmax=1)\n",
    "axs[0].set_title(\"Learned Signal (Neural Field)\", fontsize=14, fontweight='bold', color='gray')\n",
    "axs[0].axis('off')\n",
    "\n",
    "rect = patches.Rectangle((0,0), 0, 0, linewidth=2.5, edgecolor=COLOR_BOX, facecolor='none')\n",
    "axs[0].add_patch(rect)\n",
    "scat = axs[0].scatter([], [], s=4, c=COLOR_POINTS, edgecolors='none', alpha=0.6)\n",
    "\n",
    "# Initialize dummy data for dynamic plots\n",
    "dummy_data = np.zeros((SAMPLE_RES, SAMPLE_RES, 3))\n",
    "\n",
    "# 2. Naive Sampling (Aliased)\n",
    "im_naive = axs[1].imshow(dummy_data, vmin=0, vmax=1, interpolation='nearest') \n",
    "axs[1].set_title(\"Naive Sampling (Aliased)\", fontsize=14, fontweight='bold', color=COLOR_BOX)\n",
    "axs[1].axis('off')\n",
    "\n",
    "# 3. Prefiltered (Ours) - New Column\n",
    "im_filtered = axs[2].imshow(dummy_data, vmin=0, vmax=1, interpolation='nearest')\n",
    "axs[2].set_title(\"Prefiltered Input (Ours)\", fontsize=14, fontweight='bold', color='tab:green')\n",
    "axs[2].axis('off')\n",
    "\n",
    "# --- Render Loop ---\n",
    "writer = imageio.get_writer(\n",
    "    video_output, \n",
    "    fps=FPS,\n",
    "    codec='libx264', \n",
    "    quality=None, \n",
    "    pixelformat='yuv420p',\n",
    "    ffmpeg_params=[\n",
    "        '-crf', '5', \n",
    "        '-preset', 'veryslow', \n",
    "        '-tune', 'grain'\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"Rendering {FRAMES} frames...\")\n",
    "\n",
    "for i in tqdm(range(FRAMES)):\n",
    "    t = i / (FRAMES - 1)\n",
    "    \n",
    "    # Logic\n",
    "    cx, cy, span = get_trajectory_point_time(t, keyframes)\n",
    "    cx, cy = clamp_bounds(cx, cy, span, limit=0.99)\n",
    "    bounds = (cx - span/2, cy - span/2, cx + span/2, cy + span/2)\n",
    "    \n",
    "    # Generate Coords\n",
    "    coords = make_coord_grid(ndim=2, resolution=SAMPLE_RES, bounds=bounds, device=device)\n",
    "    flattened_coords = coords.view(-1, 2)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # --- A. Naive Inference (Coords Only) ---\n",
    "        naive_out = trainer.model.forward(\n",
    "            {\"coords\": flattened_coords},\n",
    "        )\n",
    "        naive_out = naive_out[\"filtered_signal\"].view(SAMPLE_RES, SAMPLE_RES, 3)\n",
    "        naive_out = naive_out.clamp(-1.0, 1.0).add(1.0).div(2.0).cpu().numpy()\n",
    "\n",
    "        # --- B. Prefiltered Inference (Coords + Covariance) ---\n",
    "        # 1. Calculate pixel width based on current zoom (span)\n",
    "        pixel_width = span / SAMPLE_RES\n",
    "        \n",
    "        # 2. Construct isotropic covariance matrix)\n",
    "        cov_val = (pixel_width / 2.0) ** 2\n",
    "        covariances = torch.eye(2, device=device).unsqueeze(0).repeat(flattened_coords.shape[0], 1, 1) * cov_val\n",
    "        \n",
    "        filtered_out = trainer.model.forward(\n",
    "            {\"coords\": flattened_coords, \"covariances\": covariances},\n",
    "        )\n",
    "        filtered_out = filtered_out[\"filtered_signal\"].view(SAMPLE_RES, SAMPLE_RES, 3)\n",
    "        filtered_out = filtered_out.clamp(-1.0, 1.0).add(1.0).div(2.0).cpu().numpy()\n",
    "\n",
    "    # Updates\n",
    "    rect.set_xy((bounds[0], bounds[1])) \n",
    "    rect.set_width(span)\n",
    "    rect.set_height(span)\n",
    "    \n",
    "    c_np = flattened_coords.cpu().numpy()\n",
    "    scat.set_offsets(c_np)\n",
    "    \n",
    "    # Image Updates\n",
    "    im_naive.set_data(naive_out)\n",
    "    im_filtered.set_data(filtered_out) # Update third column\n",
    "    \n",
    "    fig.canvas.draw()\n",
    "    frame = np.asarray(fig.canvas.buffer_rgba())\n",
    "    writer.append_data(frame)\n",
    "\n",
    "writer.close()\n",
    "plt.close()\n",
    "print(f\"Done. Saved {video_output}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spnf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
