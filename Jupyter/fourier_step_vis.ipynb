{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91967059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 10 models from scratch...\n",
      "--- Model 1/10 (LogVar: -3.50) ---\n",
      "--- Model 2/10 (LogVar: -3.89) ---\n",
      "--- Model 3/10 (LogVar: -4.28) ---\n",
      "--- Model 4/10 (LogVar: -4.67) ---\n",
      "--- Model 5/10 (LogVar: -5.06) ---\n",
      "--- Model 6/10 (LogVar: -5.44) ---\n",
      "--- Model 7/10 (LogVar: -5.83) ---\n",
      "--- Model 8/10 (LogVar: -6.22) ---\n",
      "--- Model 9/10 (LogVar: -6.61) ---\n",
      "--- Model 10/10 (LogVar: -7.00) ---\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "from typing import Union, Sequence\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.patches as patches\n",
    "from omegaconf import OmegaConf\n",
    "import hydra\n",
    "from hydra import initialize_config_dir, compose\n",
    "import time\n",
    "import imageio as iio \n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import scipy.fft \n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor \n",
    "\n",
    "import spnf\n",
    "from spnf.utils import set_seed, make_coord_grid, apply_to_tensors, to_py, interpolate_covariance_matrices_numpy\n",
    "from spnf.sample import (\n",
    "    rand_ortho, logrand, construct_covariance, sample_gaussian_delta, sample_ellipsoid_delta\n",
    ")\n",
    "from spnf.trainer import Trainer\n",
    "\n",
    "config_dir = Path(spnf.__file__).parent / \"configs\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def get_cfg(config_name, config_dir: Path, overrides: list):\n",
    "    with initialize_config_dir(version_base=None, config_dir=str(config_dir)):\n",
    "        cfg = compose(config_name=config_name, overrides=overrides)\n",
    "    return cfg\n",
    "\n",
    "def render_kernel(kind, cov_mat, interval=(-3.0, 3.0), res=1024, a=1.0, colormap=None):\n",
    "    device = cov_mat.device\n",
    "    lo, hi = float(interval[0]), float(interval[1])\n",
    "    x = torch.linspace(lo, hi, res, device=device)\n",
    "    grid = torch.stack(torch.meshgrid(x, x, indexing='xy'), dim=-1).reshape(-1, 2)\n",
    "\n",
    "    inv_cov = torch.linalg.inv(cov_mat)\n",
    "    v = grid @ inv_cov\n",
    "    q = (v * grid).sum(-1)\n",
    "\n",
    "    if kind.lower() == 'gaussian':\n",
    "        ker = torch.exp(-0.5 * q)\n",
    "        ker01 = ker.clamp(0, 1)\n",
    "    elif kind.lower() == 'uniform_ellipsoid':\n",
    "        ker = (q.sqrt() < 1).to(grid.dtype)\n",
    "        ker01 = ker\n",
    "    elif kind.lower() == 'lanczos':\n",
    "        t = q.sqrt()\n",
    "        ker = torch.sinc(t) * torch.sinc(t / a)\n",
    "        ker = ker / torch.max(ker.abs()) * 2.5\n",
    "        ker01 = ker.clamp(-1, 1).abs().clamp(0,1)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown kind\")\n",
    "\n",
    "    img_u8 = (ker01.reshape(res, res) * 255).round().to(torch.uint8).cpu().numpy()\n",
    "\n",
    "    if colormap is not None:\n",
    "        vis = cv2.applyColorMap(img_u8, colormap)\n",
    "    else:\n",
    "        vis = cv2.cvtColor(img_u8, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    return vis\n",
    "\n",
    "def image_spectrum(image: Tensor) -> Tensor:\n",
    "    img = image[0].numpy()\n",
    "    v = np.zeros_like(img)\n",
    "    v[0, :] = img[-1, :] - img[0, :]\n",
    "    v[-1, :] = img[0, :] - img[-1, :]\n",
    "    v[:, 0] += img[:, -1] - img[:, 0]\n",
    "    v[:, -1] += img[:, 0] - img[:, -1]\n",
    "    v_hat = np.fft.fftn(v)\n",
    "    M, N = v_hat.shape\n",
    "    q = np.arange(M).reshape(M, 1).astype(v_hat.dtype)\n",
    "    r = np.arange(N).reshape(1, N).astype(v_hat.dtype)\n",
    "    den = (2 * np.cos(np.divide((2 * np.pi * q), M)) + 2 * np.cos(np.divide((2 * np.pi * r), N)) - 4)\n",
    "    s = np.divide(v_hat, den, out=np.zeros_like(v_hat), where=den != 0)\n",
    "    s[0, 0] = 0\n",
    "    smooth_component = np.real(np.fft.ifftn(s))\n",
    "    magnitudes = np.abs(scipy.fft.fftshift(scipy.fft.fft2(img - smooth_component)))\n",
    "    return torch.as_tensor(magnitudes[None])\n",
    "\n",
    "def _spectrum(picture):\n",
    "    spectrum = image_spectrum(picture.mean(dim=0, keepdim=True))\n",
    "    crop = (picture.shape[1] - picture.shape[1]//4) // 2\n",
    "    return (spectrum[0, crop:-crop, crop:-crop].clamp(1e-5).log10() - 1.5).clamp(0) / 3.5 * 2 - 1\n",
    "\n",
    "def spectrum_vis(x_rgb_m1p1):\n",
    "    s = _spectrum(x_rgb_m1p1.permute(2,0,1).cpu())\n",
    "    if isinstance(s, torch.Tensor): s = s.detach().cpu()\n",
    "    s = np.asarray(s.squeeze())\n",
    "    u8 = np.clip((s + 1.0) * 127.5, 0, 255).round().astype(np.uint8)\n",
    "    return cv2.cvtColor(u8, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "def render_basis_frame(A, covariance, t_plot, proj_dir, num_basis_vis):\n",
    "    # Calculates damping and renders the basis plot for a specific covariance state\n",
    "    freqs_1d = (A @ proj_dir)\n",
    "    quad_form = (A @ covariance @ A.T).diagonal()\n",
    "    attenuation = torch.exp(-2 * (torch.pi**2) * quad_form)\n",
    "    \n",
    "    phases = freqs_1d @ t_plot.unsqueeze(0) * 2 * np.pi\n",
    "    waves = torch.cos(phases) * attenuation.unsqueeze(1)\n",
    "    \n",
    "    waves_np = waves.cpu().numpy()\n",
    "    att_np = attenuation.cpu().numpy()\n",
    "    \n",
    "    fig_basis, axes = plt.subplots(2, 5, figsize=(15, 4), sharex=True, sharey=True)\n",
    "    axes_flat = axes.flatten() \n",
    "    for b_i in range(num_basis_vis):\n",
    "        ax = axes_flat[b_i]\n",
    "        # Reference wave (gray)\n",
    "        ref_wave = torch.cos(phases[b_i]).cpu().numpy()\n",
    "        ax.plot(t_plot.cpu(), ref_wave, color='gray', alpha=0.2, linewidth=1)\n",
    "        # Attenuated wave\n",
    "        color_val = att_np[b_i]\n",
    "        ax.plot(t_plot.cpu(), waves_np[b_i], color=plt.cm.magma(color_val), linewidth=1.5)\n",
    "        ax.set_ylim(-1.2, 1.2)\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fig_basis.canvas.draw()\n",
    "    frame = np.array(fig_basis.canvas.buffer_rgba())\n",
    "    plt.close(fig_basis)\n",
    "    return frame\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Training visualization for neural fields with Spectral Prefiltering\n",
    "\n",
    "# %%\n",
    "filter_type = \"gaussian\" \n",
    "lambda_decay_start = 100 \n",
    "train_steps_per_model = 350\n",
    "num_models = 10\n",
    "SRC_RES = 512\n",
    "FPS = 30                 \n",
    "\n",
    "# Covariance range: -1.5 (Blurry) to -5.0 (Sharp)\n",
    "cov_logvars = np.linspace(-3.5, -7.0, num_models)\n",
    "\n",
    "cfg = get_cfg(\n",
    "    config_name=\"train\",\n",
    "    config_dir=config_dir,\n",
    "    overrides=[\n",
    "        f\"data.grid.resize={SRC_RES}\",\n",
    "        \"data.bounds=1.0\", \n",
    "        \"tensorboard=null\",\n",
    "        f\"trainer.steps={train_steps_per_model}\",\n",
    "        \"paths.output_dir=/home/myaldiz/Data/Experiments/spnf/${task_name}\",\n",
    "        f\"scheduler.lr_lambda.decay_start={lambda_decay_start}\",\n",
    "        \"trainer.compile_train_step=True\",\n",
    "        \"trainer.mc_samples_train=0\",\n",
    "        \"model.encoder.length_distribution_param=500.0\"\n",
    "    ],\n",
    ")\n",
    "\n",
    "video_settings = dict(fps=FPS, codec='libx265', pixelformat='yuv420p')\n",
    "ffmpeg_params = [\n",
    "    '-crf', '20','-preset', 'fast', '-tag:v', 'hvc1', '-tune', 'grain'\n",
    "]\n",
    "\n",
    "output_dir = Path(cfg.paths.output_dir)\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "video_output_dir = output_dir / \"visualizations\" \n",
    "video_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# %%\n",
    "class TrainerModified(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.current_covariance = None\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate_data_train(self, coords=None, eigenvalues=None, eigenvectors=None, perturb_kernel=None) -> dict[str, torch.Tensor]:\n",
    "        batch = super().generate_data_train(coords, eigenvalues, eigenvectors, perturb_kernel)\n",
    "        if self.current_covariance is not None:\n",
    "            batch_size = batch[\"coords\"].shape[0]\n",
    "            batch[\"covariances\"] = self.current_covariance.expand(batch_size, -1, -1)\n",
    "            batch[\"filter_type\"] = filter_type\n",
    "        return batch\n",
    "\n",
    "final_frames_main = []\n",
    "# We will store the covariance matrices to interpolate them later for the basis video\n",
    "saved_covariances = [] \n",
    "\n",
    "# --- Setup Basis Selection ---\n",
    "temp_trainer = TrainerModified(cfg).to(device)\n",
    "num_basis_vis = 10\n",
    "basis_matrix = temp_trainer.model.encoder.A \n",
    "magnitudes = basis_matrix.norm(dim=1)\n",
    "sorted_indices = torch.argsort(magnitudes)\n",
    "selected_indices = sorted_indices[torch.linspace(0, len(magnitudes)-1, num_basis_vis).long()]\n",
    "# Store A matrix for rendering later\n",
    "vis_A = basis_matrix[selected_indices]\n",
    "\n",
    "t_plot = torch.linspace(-0.5, 0.5, 200, device=device)\n",
    "proj_dir = torch.randn(2, 1, device=device)\n",
    "proj_dir = F.normalize(proj_dir, dim=0)\n",
    "del temp_trainer \n",
    "\n",
    "# --- Main Multi-Model Loop ---\n",
    "print(f\"Training {num_models} models from scratch...\")\n",
    "\n",
    "for model_idx, log_var in enumerate(cov_logvars):\n",
    "    print(f\"--- Model {model_idx+1}/{num_models} (LogVar: {log_var:.2f}) ---\")\n",
    "    \n",
    "    trainer = TrainerModified(cfg).to(device)\n",
    "    \n",
    "    # Construct Covariance\n",
    "    cov_val = 10**log_var\n",
    "    cov_mat = torch.eye(2, device=device) * cov_val\n",
    "    trainer.current_covariance = cov_mat\n",
    "    \n",
    "    # Save for later interpolation\n",
    "    saved_covariances.append(cov_mat)\n",
    "    \n",
    "    trainer.fit(num_steps=train_steps_per_model, no_tqdm=True)\n",
    "    \n",
    "    # Generate Prediction Frame\n",
    "    output = trainer.generate_grid_data(resolution=SRC_RES)\n",
    "    pred_img = output[\"filtered_signal\"].clamp(-1.0, 1.0).add(1.0).div(2.0).cpu().numpy()\n",
    "    \n",
    "    if pred_img.ndim == 3 and pred_img.shape[0] in [1, 3]:\n",
    "        pred_img = np.transpose(pred_img, (1, 2, 0))\n",
    "        \n",
    "    # Insets\n",
    "    kern_img = render_kernel(kind=filter_type, cov_mat=cov_mat, interval=(-0.25, 0.25), res=256)\n",
    "    pred_tensor_m1p1 = (torch.from_numpy(pred_img) * 2.0) - 1.0 \n",
    "    spect_img = spectrum_vis(pred_tensor_m1p1)\n",
    "    \n",
    "    # Compose Main Frame \n",
    "    h, w = pred_img.shape[:2]\n",
    "    fig_width = 12 \n",
    "    img_height_in = fig_width * (h / w)\n",
    "    \n",
    "    fig = plt.figure(figsize=(fig_width, img_height_in), dpi=100)\n",
    "    ax_img = fig.add_axes([0, 0, 1, 1])\n",
    "    ax_img.imshow(pred_img)\n",
    "    ax_img.axis('off')\n",
    "    \n",
    "    inset_size = 0.25 \n",
    "    \n",
    "    # Top Right: Kernel\n",
    "    ax_kern = fig.add_axes([0.98 - inset_size, 0.98 - inset_size * (fig_width/img_height_in), inset_size, inset_size * (fig_width/img_height_in)])\n",
    "    center = kern_img.shape[0] // 2\n",
    "    q_size = kern_img.shape[0] // 4\n",
    "    kern_zoom = kern_img[center - q_size:center + q_size, center - q_size:center + q_size]\n",
    "    ax_kern.imshow(kern_zoom)\n",
    "    ax_kern.set_xticks([]); ax_kern.set_yticks([])\n",
    "    for spine in ax_kern.spines.values(): spine.set_color('white'); spine.set_linewidth(2)\n",
    "    ax_kern.set_title(\"Spatial Kernel\", fontsize=12, color='white', backgroundcolor='black')\n",
    "\n",
    "    # Top Left: Spectrum\n",
    "    ax_spect = fig.add_axes([0.02, 0.98 - inset_size * (fig_width/img_height_in), inset_size, inset_size * (fig_width/img_height_in)])\n",
    "    ax_spect.imshow(spect_img)\n",
    "    ax_spect.set_xticks([]); ax_spect.set_yticks([])\n",
    "    for spine in ax_spect.spines.values(): spine.set_color('white'); spine.set_linewidth(2)\n",
    "    ax_spect.set_title(\"Spectrum\", fontsize=12, color='white', backgroundcolor='black')\n",
    "\n",
    "    fig.canvas.draw()\n",
    "    main_frame = np.array(fig.canvas.buffer_rgba())\n",
    "    final_frames_main.append(main_frame)\n",
    "    plt.close(fig)\n",
    "    \n",
    "    del trainer\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "696cf0f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rendering interpolated videos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (1500, 400) to (1504, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "x265 [info]: HEVC encoder version 3.5+1-f0c1022b6\n",
      "x265 [info]: build info [Linux][GCC 8.3.0][64 bit] 8bit+10bit+12bit\n",
      "x265 [info]: using cpu capabilities: MMX2 SSE2Fast LZCNT SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2\n",
      "x265 [info]: Main profile, Level-4 (Main tier)\n",
      "x265 [info]: Thread pool created using 24 threads\n",
      "x265 [info]: Slices                              : 1\n",
      "x265 [info]: frame threads / pool features       : 4 / wpp(19 rows)\n",
      "x265 [info]: Coding QT: max CU size, min CU size : 64 / 8\n",
      "x265 [info]: Residual QT: max TU size, max depth : 32 / 1 inter / 1 intra\n",
      "x265 [info]: ME / range / subpel / merge         : hex / 57 / 2 / 2\n",
      "x265 [info]: Keyframe min / max / scenecut / bias  : 25 / 250 / 40 / 5.00 \n",
      "x265 [info]: Lookahead / bframes / badapt        : 15 / 4 / 0\n",
      "x265 [info]: b-pyramid / weightp / weightb       : 1 / 1 / 0\n",
      "x265 [info]: References / ref-limit  cu / depth  : 3 / on / on\n",
      "x265 [info]: Rate Control / qCompress            : CRF-20.0 / 0.60\n",
      "x265 [info]: tools: rd=2 psy-rd=4.00 signhide tmvp fast-intra\n",
      "x265 [info]: tools: strong-intra-smoothing lslices=7 deblock\n",
      "x265 [info]: HEVC encoder version 3.5+1-f0c1022b6\n",
      "x265 [info]: build info [Linux][GCC 8.3.0][64 bit] 8bit+10bit+12bit\n",
      "x265 [info]: using cpu capabilities: MMX2 SSE2Fast LZCNT SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2\n",
      "x265 [info]: Main profile, Level-3.1 (Main tier)\n",
      "x265 [info]: Thread pool created using 24 threads\n",
      "x265 [info]: Slices                              : 1\n",
      "x265 [info]: frame threads / pool features       : 4 / wpp(7 rows)\n",
      "x265 [warning]: Source height < 720p; disabling lookahead-slices\n",
      "x265 [info]: Coding QT: max CU size, min CU size : 64 / 8\n",
      "x265 [info]: Residual QT: max TU size, max depth : 32 / 1 inter / 1 intra\n",
      "x265 [info]: ME / range / subpel / merge         : hex / 57 / 2 / 2\n",
      "x265 [info]: Keyframe min / max / scenecut / bias  : 25 / 250 / 40 / 5.00 \n",
      "x265 [info]: Lookahead / bframes / badapt        : 15 / 4 / 0\n",
      "x265 [info]: b-pyramid / weightp / weightb       : 1 / 1 / 0\n",
      "x265 [info]: References / ref-limit  cu / depth  : 3 / on / on\n",
      "x265 [info]: Rate Control / qCompress            : CRF-20.0 / 0.60\n",
      "x265 [info]: tools: rd=2 psy-rd=4.00 signhide tmvp fast-intra\n",
      "x265 [info]: tools: strong-intra-smoothing deblock\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "x265 [info]: frame I:      2, Avg QP:15.50  kb/s: 69421.20\n",
      "x265 [info]: frame P:     83, Avg QP:16.25  kb/s: 4978.46 \n",
      "x265 [info]: frame B:    335, Avg QP:16.22  kb/s: 623.98  \n",
      "x265 [info]: Weighted P-Frames: Y:15.7% UV:9.6%\n",
      "x265 [info]: consecutive B-frames: 1.2% 0.0% 0.0% 1.2% 97.6% \n",
      "\n",
      "encoded 420 frames in 49.03s (8.57 fps), 1812.12 kb/s, Avg QP:16.22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main video saved to: /home/myaldiz/Data/Experiments/spnf/spnf-oia/visualizations/fourier_vis_sweep.mp4\n",
      "Basis video saved to: /home/myaldiz/Data/Experiments/spnf/spnf-oia/visualizations/fourier_basis_vis_sweep.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "x265 [info]: frame I:      2, Avg QP:16.50  kb/s: 6334.44 \n",
      "x265 [info]: frame P:     83, Avg QP:14.86  kb/s: 1497.65 \n",
      "x265 [info]: frame B:    335, Avg QP:14.86  kb/s: 295.78  \n",
      "x265 [info]: Weighted P-Frames: Y:51.8% UV:37.3%\n",
      "x265 [info]: consecutive B-frames: 1.2% 0.0% 0.0% 1.2% 97.6% \n",
      "\n",
      "encoded 420 frames in 49.71s (8.45 fps), 562.05 kb/s, Avg QP:14.87\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# --- Interpolation & Video Generation ---\n",
    "\n",
    "video_main_path = str(video_output_dir / \"fourier_vis_sweep.mp4\")\n",
    "video_basis_path = str(video_output_dir / \"fourier_basis_vis_sweep.mp4\")\n",
    "\n",
    "writer_main = iio.get_writer(video_main_path, **video_settings, ffmpeg_params=ffmpeg_params)\n",
    "writer_basis = iio.get_writer(video_basis_path, **video_settings, ffmpeg_params=ffmpeg_params)\n",
    "\n",
    "frames_per_hold = 15        \n",
    "frames_per_transition = 30  \n",
    "\n",
    "print(\"Rendering interpolated videos...\")\n",
    "\n",
    "for i in range(len(final_frames_main)):\n",
    "    current_main = final_frames_main[i]\n",
    "    current_cov = saved_covariances[i]\n",
    "    \n",
    "    # Generate the Basis frame for the *current* covariance exactly once for the hold\n",
    "    current_basis = render_basis_frame(vis_A, current_cov, t_plot, proj_dir, num_basis_vis)\n",
    "    \n",
    "    # 1. HOLD (Wait 1 second)\n",
    "    for _ in range(frames_per_hold):\n",
    "        writer_main.append_data(current_main)\n",
    "        writer_basis.append_data(current_basis)\n",
    "        \n",
    "    # 2. TRANSITION (Interpolate 1 second)\n",
    "    if i < len(final_frames_main) - 1:\n",
    "        next_main = final_frames_main[i+1]\n",
    "        next_cov = saved_covariances[i+1]\n",
    "        \n",
    "        for t in range(frames_per_transition):\n",
    "            alpha = t / float(frames_per_transition - 1)\n",
    "            \n",
    "            # A. Main Video: Standard Pixel Cross Dissolve\n",
    "            interp_main = cv2.addWeighted(current_main, 1 - alpha, next_main, alpha, 0)\n",
    "            writer_main.append_data(interp_main)\n",
    "            \n",
    "            # B. Basis Video: Mathematical Interpolation of Covariance\n",
    "            # Linear interpolation of the covariance matrix itself\n",
    "            interp_cov = (1 - alpha) * current_cov + alpha * next_cov\n",
    "            \n",
    "            # Render a FRESH frame using the interpolated covariance\n",
    "            interp_basis = render_basis_frame(vis_A, interp_cov, t_plot, proj_dir, num_basis_vis)\n",
    "            writer_basis.append_data(interp_basis)\n",
    "\n",
    "writer_main.close()\n",
    "writer_basis.close()\n",
    "print(f\"Main video saved to: {video_main_path}\")\n",
    "print(f\"Basis video saved to: {video_basis_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spnf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
