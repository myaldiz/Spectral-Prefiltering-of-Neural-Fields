{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "facba287",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Sequence\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.patches as patches\n",
    "from omegaconf import OmegaConf\n",
    "import hydra\n",
    "from hydra import initialize_config_dir, compose\n",
    "import time\n",
    "import imageio\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import spnf\n",
    "from spnf.utils import set_seed, make_coord_grid, apply_to_tensors, to_py\n",
    "from spnf.sample import (\n",
    "    rand_ortho, logrand, construct_covariance, sample_gaussian_delta, sample_ellipsoid_delta\n",
    ")\n",
    "\n",
    "config_dir = Path(spnf.__file__).parent / \"configs\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a0a356b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cfg(config_name, config_dir: Path, overrides: list):\n",
    "    with initialize_config_dir(version_base=None, config_dir=str(config_dir)):\n",
    "        cfg = compose(config_name=config_name, overrides=overrides)\n",
    "    return cfg\n",
    "\n",
    "class Trainer(torch.nn.Module):\n",
    "    def __init__(self, cfg) -> None:\n",
    "        super().__init__()\n",
    "        set_seed(cfg.trainer.seed)\n",
    "        self.cfg = cfg\n",
    "        self.model = hydra.utils.instantiate(cfg.model)\n",
    "        self.data = hydra.utils.instantiate(cfg.data)\n",
    "        self.optimizer = hydra.utils.instantiate(cfg.optimizer, params=self.model.parameters())\n",
    "        self.scheduler = hydra.utils.instantiate(cfg.scheduler, optimizer=self.optimizer)\n",
    "        if cfg.get(\"tensorboard\", None) is not None:\n",
    "            self.writer = hydra.utils.instantiate(cfg.tensorboard)\n",
    "        self.global_step = 0\n",
    "        \n",
    "    @property\n",
    "    def device(self):\n",
    "        return self.model.device\n",
    "    \n",
    "    def state_dict(self):\n",
    "        state_dict = dict(\n",
    "            model=self.model.state_dict(),\n",
    "            global_step=self.global_step,\n",
    "            optimizer=self.optimizer.state_dict(),\n",
    "        )\n",
    "        if self.scheduler:\n",
    "            state_dict[\"scheduler\"] = self.scheduler.state_dict()\n",
    "        return state_dict\n",
    "    \n",
    "    def load_state_dict(self, state_dict):\n",
    "        self.model.load_state_dict(state_dict[\"model\"])\n",
    "        self.global_step = state_dict[\"global_step\"]\n",
    "        self.optimizer.load_state_dict(state_dict[\"optimizer\"])\n",
    "        if self.scheduler and \"scheduler\" in state_dict:\n",
    "            self.scheduler.load_state_dict(state_dict[\"scheduler\"])\n",
    "        \n",
    "    def generate_training_data(self):\n",
    "        batch = {}\n",
    "        coords = self.data.generate_coords(self.cfg.trainer.batch_size)\n",
    "        batch['coords'] = coords\n",
    "        \n",
    "        # Perturb the coordinates during training\n",
    "        if self.cfg.trainer.perturb_coords is not None:\n",
    "            eigenvalues = logrand(\n",
    "                self.cfg.trainer.covariance_eigenvalue_logrange[0],\n",
    "                self.cfg.trainer.covariance_eigenvalue_logrange[1],\n",
    "                (self.cfg.trainer.batch_size, self.model.input_dim),\n",
    "                device=coords.device\n",
    "            )\n",
    "            eigenvectors = rand_ortho(\n",
    "                self.model.input_dim,\n",
    "                self.cfg.trainer.batch_size,\n",
    "                device=coords.device,\n",
    "            )\n",
    "            batch[\"covariances\"] = construct_covariance(eigenvectors, eigenvalues)\n",
    "            \n",
    "            if self.cfg.trainer.perturb_coords == \"gaussian\":\n",
    "                deltas = sample_gaussian_delta(eigenvectors, eigenvalues)\n",
    "            elif self.cfg.trainer.perturb_coords == \"uniform_ellipsoid\":\n",
    "                deltas = sample_ellipsoid_delta(eigenvectors, eigenvalues)\n",
    "            elif self.cfg.trainer.perturb_coords == \"lanczos\":\n",
    "                raise NotImplementedError(\"Lanczos sampling not implemented yet.\")\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown perturbation type: {self.cfg.trainer.perturb_coords}\")\n",
    "            \n",
    "            batch['deltas'] = deltas\n",
    "            \n",
    "            coords = coords + deltas\n",
    "                \n",
    "        return batch | dict(gt_signal=self.data(coords))\n",
    "    \n",
    "    def train_step(self) -> dict[str, torch.Tensor]:\n",
    "        batch = self.generate_training_data()\n",
    "        \n",
    "        pred = self.model(batch)\n",
    "        loss = self.model.loss(pred, batch)\n",
    "\n",
    "        loss_sum = sum(loss.values())\n",
    "        self.optimizer.zero_grad()\n",
    "        loss_sum.backward()\n",
    "        self.optimizer.step()\n",
    "        if self.scheduler:\n",
    "            self.scheduler.step()\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def generate_grid_data(self, resolution: Union[int, Sequence[int]]=512, bounds=1.0, **kwargs) -> dict[str, torch.Tensor]:\n",
    "        # Setup grid resolution\n",
    "        if isinstance(resolution, int):\n",
    "            grid_res = (resolution,) * self.model.input_dim\n",
    "        else:\n",
    "            grid_res = tuple(resolution)\n",
    "        \n",
    "        \n",
    "        # Create coordinate grid\n",
    "        coord_grid = make_coord_grid(\n",
    "            self.model.input_dim,\n",
    "            resolution=grid_res,\n",
    "            bounds=bounds,\n",
    "            device=self.device,\n",
    "        )\n",
    "        \n",
    "        # Flatten coordinate grid\n",
    "        total_pixels = coord_grid.shape[:-1].numel() \n",
    "        flattened_coords = coord_grid.view(total_pixels, self.model.input_dim)\n",
    "        \n",
    "        # Inference\n",
    "        with torch.no_grad():\n",
    "            output = self.model.forward(\n",
    "                {\"coords\": flattened_coords} | kwargs\n",
    "            )\n",
    "            \n",
    "        def reshape_if_matching(t: torch.Tensor):\n",
    "            if t.shape[0] == total_pixels:\n",
    "                new_shape = grid_res + t.shape[1:]\n",
    "                return t.view(new_shape)\n",
    "            return t\n",
    "            \n",
    "        return apply_to_tensors(output, reshape_if_matching)\n",
    "\n",
    "    def fit(self, num_steps=None, no_tqdm=False) -> dict:\n",
    "        # 1. Compile train step\n",
    "        if self.cfg.trainer.compile_train_step:\n",
    "            train_step = torch.compile(self.train_step)\n",
    "        else:\n",
    "            train_step = self.train_step\n",
    "        \n",
    "        if num_steps is None:\n",
    "            num_steps = self.cfg.trainer.steps\n",
    "\n",
    "        pbar = tqdm(total=num_steps, desc=\"Training\", dynamic_ncols=True, disable=no_tqdm)\n",
    "        \n",
    "        stats_history = []\n",
    "        for _ in range(num_steps):\n",
    "            stats = train_step()\n",
    "            self.global_step += 1\n",
    "            \n",
    "            # Convert tensors to python scalars/lists\n",
    "            step_stats = {k: to_py(v) for k, v in stats.items()}\n",
    "            stats_history.append(step_stats)\n",
    "\n",
    "            # Update progress bar (using the raw tensor items for display is fine/fast)\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix({k: f\"{v:.4f}\"  for k, v in step_stats.items() if isinstance(v, float)})\n",
    "\n",
    "        pbar.close()\n",
    "        \n",
    "        # Concatenate (Transpose)\n",
    "        if stats_history:\n",
    "            final_stats = {k: [step[k] for step in stats_history] for k in stats_history[0]}\n",
    "        else:\n",
    "            final_stats = {}\n",
    "            \n",
    "        return final_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6151715",
   "metadata": {},
   "source": [
    "## Training visualization for neural fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48a04282",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_decay_start = 500\n",
    "vis_steps = 800\n",
    "total_steps = 2000\n",
    "SRC_RES = 512\n",
    "cfg = get_cfg(\n",
    "    config_name=\"train\",\n",
    "    config_dir=config_dir,\n",
    "    overrides=[\n",
    "        f\"data.grid.resize={SRC_RES}\",\n",
    "        \"data.bounds=1.0\", \n",
    "        \"tensorboard=null\",\n",
    "        f\"trainer.steps={total_steps}\",\n",
    "        \"paths.output_dir=/home/myaldiz/Data/Experiments/spnf/${task_name}\",\n",
    "        f\"scheduler.lr_lambda.decay_start={lambda_decay_start}\",\n",
    "        \"trainer.compile_train_step=False\",\n",
    "        \"trainer.perturb_coords=null\"\n",
    "    ],\n",
    ")\n",
    "# Create output directories\n",
    "output_dir = Path(cfg.paths.output_dir)\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "checkpoint_dir = Path(cfg.trainer.checkpoint_dir)\n",
    "checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save the config to output directory\n",
    "OmegaConf.save(config=cfg, f=output_dir / \"config_singlescale.yaml\", resolve=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fe1f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(cfg).to(device)\n",
    "\n",
    "loss_average = []\n",
    "predictions = []\n",
    "steps_history = []  # New: Track the actual step number for the X-axis\n",
    "\n",
    "current_step = 0\n",
    "frame_count = 0\n",
    "pbar = tqdm(total=vis_steps, desc=\"Training\")\n",
    "\n",
    "while current_step < vis_steps:\n",
    "    # Determine stride based on how many frames we have generated\n",
    "    if current_step < 90:\n",
    "        stride = 1\n",
    "    elif current_step < 210:\n",
    "        stride = 2\n",
    "    else:\n",
    "        stride = 5\n",
    "        \n",
    "    # Ensure we don't exceed vis_steps\n",
    "    if current_step + stride > vis_steps:\n",
    "        stride = vis_steps - current_step\n",
    "        if stride == 0: break\n",
    "\n",
    "    # Fit for 'stride' steps\n",
    "    stats = trainer.fit(num_steps=stride, no_tqdm=True)\n",
    "    \n",
    "    # Update trackers\n",
    "    current_step += stride\n",
    "    frame_count += 1\n",
    "    pbar.update(stride)\n",
    "\n",
    "    # Store Data\n",
    "    loss_average.append(np.mean(stats['mse_loss']))\n",
    "    steps_history.append(current_step) # Store actual X-axis location\n",
    "    \n",
    "    # Generate visualization\n",
    "    output = trainer.generate_grid_data(resolution=SRC_RES)\n",
    "    predictions.append(output[\"filtered_signal\"].clamp(-1.0, 1.0).add(1.0).div(2.0).cpu().numpy())\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "# --- VIDEO GENERATION ---\n",
    "video_output = output_dir / \"visualizations\" / \"singlescale_train_vis.mp4\"\n",
    "video_output.parent.mkdir(parents=True, exist_ok=True)\n",
    "writer = imageio.get_writer(\n",
    "    video_output, \n",
    "    fps=30, \n",
    "    codec='libx264', \n",
    "    quality=None, \n",
    "    pixelformat='yuv420p',\n",
    "    ffmpeg_params=[\n",
    "        '-crf', '18', \n",
    "        '-preset', 'veryslow', \n",
    "        '-tune', 'grain'\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Settings\n",
    "plt.style.use('default') \n",
    "max_loss = max(loss_average) * 1.1\n",
    "\n",
    "# 1. Geometry Calculation\n",
    "sample = predictions[0]\n",
    "if sample.ndim == 3 and sample.shape[0] in [1, 3]:\n",
    "    h, w = sample.shape[1], sample.shape[2]\n",
    "else:\n",
    "    h, w = sample.shape[0], sample.shape[1]\n",
    "\n",
    "fig_width = 12 \n",
    "img_height_in = fig_width * (h / w)\n",
    "loss_height_in = 4.0 \n",
    "total_height = img_height_in + loss_height_in\n",
    "\n",
    "# Calculate relative height ratios for manual positioning\n",
    "h_img_ratio = img_height_in / total_height\n",
    "h_loss_ratio = loss_height_in / total_height\n",
    "\n",
    "print(\"Rendering video frames...\")\n",
    "# Zip with steps_history to get the correct X-axis value\n",
    "for i, (loss, pred, step_num) in enumerate(tqdm(zip(loss_average, predictions, steps_history), total=len(predictions))):\n",
    "    \n",
    "    # Create figure without subplots/layouts to allow manual placement\n",
    "    fig = plt.figure(figsize=(fig_width, total_height), dpi=100)\n",
    "    \n",
    "    # Bottom Plot\n",
    "    margin_bottom = 0.08\n",
    "    h_loss_actual = h_loss_ratio - margin_bottom - 0.02\n",
    "    ax_loss = fig.add_axes([0.12, margin_bottom, 0.82, h_loss_actual])\n",
    "    \n",
    "    margin_top = 0.02\n",
    "    y_img_start = h_loss_ratio + 0.02\n",
    "    h_img_actual = h_img_ratio - margin_top - 0.02\n",
    "    ax_img = fig.add_axes([0.02, y_img_start, 0.96, h_img_actual])\n",
    "\n",
    "    # --- 1. Image Plot ---\n",
    "    if pred.ndim == 3 and pred.shape[0] in [1, 3]: \n",
    "        pred = np.transpose(pred, (1, 2, 0))\n",
    "    \n",
    "    ax_img.imshow(pred, aspect='auto') \n",
    "    \n",
    "    # Hide all ticks/spines for the image\n",
    "    ax_img.set_xticks([])\n",
    "    ax_img.set_yticks([])\n",
    "    for spine in ax_img.spines.values():\n",
    "        spine.set_visible(False) \n",
    "    \n",
    "    # --- 2. Loss Plot ---\n",
    "    # MODIFICATION: Use steps_history for X-axis data\n",
    "    current_steps_data = steps_history[:i+1]\n",
    "    current_loss_data = loss_average[:i+1]\n",
    "    \n",
    "    ax_loss.plot(current_steps_data, current_loss_data, color='tab:blue', linewidth=3)\n",
    "    \n",
    "    # MODIFICATION: Use step_num (the actual training step) for the scatter X-coordinate\n",
    "    ax_loss.scatter(step_num, loss, color='tab:red', s=100, zorder=5) \n",
    "    \n",
    "    # MODIFICATION: Set X limit to the total vis_steps (800) rather than frame count\n",
    "    ax_loss.set_xlim(0, vis_steps)\n",
    "    ax_loss.set_ylim(0, max_loss)\n",
    "    \n",
    "    ax_loss.set_ylabel('MSE Loss', fontsize=24, labelpad=15, fontweight='medium')\n",
    "    ax_loss.set_xlabel('Steps', fontsize=18) # Optional: Label x-axis\n",
    "    ax_loss.tick_params(axis='both', which='major', labelsize=16)\n",
    "    ax_loss.grid(True, linestyle='--', alpha=0.5, linewidth=1.5)\n",
    "\n",
    "    # --- 3. Save Frame ---\n",
    "    fig.canvas.draw()\n",
    "    frame = np.array(fig.canvas.buffer_rgba())\n",
    "    \n",
    "    writer.append_data(frame)\n",
    "    plt.close(fig)\n",
    "\n",
    "writer.close()\n",
    "print(f\"Video saved to {video_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8054ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train remaining steps\n",
    "trainer.fit(num_steps=total_steps - vis_steps)\n",
    "\n",
    "# Save the state dict\n",
    "state_dict = trainer.state_dict()\n",
    "checkpoint_path = checkpoint_dir / \"singlescale.pth\"\n",
    "torch.save(state_dict, str(checkpoint_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb6ca53",
   "metadata": {},
   "source": [
    "## Visualize aliasing artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a2856b",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = checkpoint_dir / \"singlescale.pth\"\n",
    "if not checkpoint_path.exists():\n",
    "    # Fit the model\n",
    "    trainer.fit()\n",
    "\n",
    "    # Save the state dict\n",
    "    state_dict = trainer.state_dict()\n",
    "    torch.save(state_dict, str(checkpoint_path))\n",
    "else:\n",
    "    # Load the state dict\n",
    "    trainer = Trainer(cfg).to(device)\n",
    "    state_dict = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "    trainer.load_state_dict(state_dict)\n",
    "    print(f\"Loaded model from {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d69de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ Visualization Script ============\n",
    "COLOR_BOX = '#D46CCD'\n",
    "COLOR_POINTS = '#4E71BE'\n",
    "SAMPLE_RES = 32\n",
    "FPS = 30\n",
    "SECONDS = 10\n",
    "FRAMES = FPS * SECONDS\n",
    "video_output = output_dir / \"visualizations\" / \"singlescale_aliasing_vis.mp4\"\n",
    "\n",
    "# --- Keyframes (Fixed Timing) ---\n",
    "# Format: (Time_Percent, Center_X, Center_Y, Span)\n",
    "keyframes = [\n",
    "    (0.00, -0.5,  0.5, 0.4), # Start Top-Left\n",
    "    (0.40,  0.5,  0.5, 0.4), # Move Right\n",
    "    (0.50,  0.5, -0.5, 1.0), # Zoom Out & Down\n",
    "    (0.90, -0.5, -0.5, 1.0), # Move Left (Zoomed Out)\n",
    "    (1.00, -0.5,  0.5, 0.4), # Zoom In & Up\n",
    "]\n",
    "\n",
    "def get_trajectory_point_time(t_global, keyframes):\n",
    "    for k in range(len(keyframes) - 1):\n",
    "        t0, x0, y0, s0 = keyframes[k]\n",
    "        t1, x1, y1, s1 = keyframes[k+1]\n",
    "        if t0 <= t_global <= t1 + 1e-5:\n",
    "            segment_duration = t1 - t0\n",
    "            if segment_duration <= 1e-5: return x1, y1, s1\n",
    "            local_t = np.clip((t_global - t0) / segment_duration, 0, 1)\n",
    "            smooth_t = local_t * local_t * (3 - 2 * local_t) \n",
    "            current_x = x0 + (x1 - x0) * smooth_t\n",
    "            current_y = y0 + (y1 - y0) * smooth_t\n",
    "            current_s = s0 + (s1 - s0) * smooth_t\n",
    "            return current_x, current_y, current_s\n",
    "    return keyframes[-1][1:]\n",
    "\n",
    "def clamp_bounds(cx, cy, span, limit=1.0):\n",
    "    half_s = span / 2.0\n",
    "    if cx - half_s < -limit: cx = -limit + half_s\n",
    "    if cx + half_s > limit:  cx = limit - half_s\n",
    "    if cy - half_s < -limit: cy = -limit + half_s\n",
    "    if cy + half_s > limit:  cy = limit - half_s\n",
    "    return cx, cy\n",
    "\n",
    "# --- Setup Plot ---\n",
    "plt.style.use('seaborn-v0_8-white') \n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5.5), dpi=200, constrained_layout=True)\n",
    "fig.patch.set_facecolor('white')\n",
    "\n",
    "# Context View\n",
    "full_field = trainer.data\n",
    "output = trainer.generate_grid_data(resolution=SRC_RES)\n",
    "reconstructed_data = output[\"filtered_signal\"].clamp(-1.0, 1.0).add(1.0).div(2.0).cpu().numpy()\n",
    "im_context = axs[0].imshow(reconstructed_data, extent=(-1, 1, 1, -1), cmap='gray', vmin=0, vmax=1)\n",
    "axs[0].set_title(\"Learned Signal (Neural Field)\", fontsize=14, fontweight='bold', color='gray')\n",
    "axs[0].axis('off')\n",
    "\n",
    "rect = patches.Rectangle((0,0), 0, 0, linewidth=2.5, edgecolor=COLOR_BOX, facecolor='none')\n",
    "axs[0].add_patch(rect)\n",
    "\n",
    "# FIX 1: Smaller points (s=4) and alpha=0.6 for better visibility without occlusion\n",
    "scat = axs[0].scatter([], [], s=4, c=COLOR_POINTS, edgecolors='none', alpha=0.6)\n",
    "\n",
    "# Naive & Ours Views\n",
    "dummy_data = np.zeros((SAMPLE_RES, SAMPLE_RES, 3))\n",
    "im_naive = axs[1].imshow(dummy_data, vmin=0, vmax=1, interpolation='nearest') \n",
    "axs[1].set_title(\"Naive Sampling (Aliased)\", fontsize=14, fontweight='bold', color=COLOR_BOX)\n",
    "axs[1].axis('off')\n",
    "\n",
    "# --- Render Loop ---\n",
    "# FIX 2: High Quality Compression settings\n",
    "# -crf 18: Visually lossless (lower is better quality, 0 is lossless)\n",
    "# -tune grain: Preserves high frequency noise/flicker\n",
    "# -preset veryslow: Best compression efficiency for the file size\n",
    "writer = imageio.get_writer(\n",
    "    video_output, \n",
    "    fps=FPS,\n",
    "    codec='libx264', \n",
    "    quality=None, \n",
    "    pixelformat='yuv420p',\n",
    "    ffmpeg_params=[\n",
    "        '-crf', '18', \n",
    "        '-preset', 'veryslow', \n",
    "        '-tune', 'grain'\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"Rendering {FRAMES} frames...\")\n",
    "\n",
    "for i in tqdm(range(FRAMES)):\n",
    "    t = i / (FRAMES - 1)\n",
    "    \n",
    "    # Logic\n",
    "    cx, cy, span = get_trajectory_point_time(t, keyframes)\n",
    "    cx, cy = clamp_bounds(cx, cy, span, limit=0.99)\n",
    "    bounds = (cx - span/2, cy - span/2, cx + span/2, cy + span/2)\n",
    "    coords = make_coord_grid(ndim=2, resolution=SAMPLE_RES, bounds=bounds, device=device)\n",
    "\n",
    "    # Naive\n",
    "    with torch.no_grad():\n",
    "        naive_out = trainer.model.forward(\n",
    "            {\"coords\": coords.view(-1, 2)},\n",
    "        )\n",
    "        naive_out = naive_out[\"filtered_signal\"].view(SAMPLE_RES, SAMPLE_RES, 3)\n",
    "        naive_out = naive_out.clamp(-1.0, 1.0).add(1.0).div(2.0).cpu().numpy()\n",
    "\n",
    "    # Updates\n",
    "    rect.set_xy((bounds[0], bounds[1])) \n",
    "    rect.set_width(span)\n",
    "    rect.set_height(span)\n",
    "    \n",
    "    c_np = coords.view(-1,2).cpu().numpy()\n",
    "    scat.set_offsets(c_np)\n",
    "    \n",
    "    # Image Updates\n",
    "    im_naive.set_data(naive_out)\n",
    "    \n",
    "    fig.canvas.draw()\n",
    "    frame = np.asarray(fig.canvas.buffer_rgba())\n",
    "    writer.append_data(frame)\n",
    "\n",
    "writer.close()\n",
    "plt.close()\n",
    "print(f\"Done. Saved {video_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f3acac",
   "metadata": {},
   "source": [
    "## Multiscale training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b79c660",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_decay_start = 500\n",
    "vis_steps = 800\n",
    "total_steps = 2000\n",
    "SRC_RES = 512\n",
    "cfg = get_cfg(\n",
    "    config_name=\"train\",\n",
    "    config_dir=config_dir,\n",
    "    overrides=[\n",
    "        f\"data.grid.resize={SRC_RES}\",\n",
    "        \"data.bounds=1.0\", \n",
    "        \"tensorboard=null\",\n",
    "        f\"trainer.steps={total_steps}\",\n",
    "        \"paths.output_dir=/home/myaldiz/Data/Experiments/spnf/${task_name}\",\n",
    "        f\"scheduler.lr_lambda.decay_start={lambda_decay_start}\",\n",
    "        \"trainer.compile_train_step=True\",\n",
    "        \"trainer.perturb_coords=gaussian\"\n",
    "    ],\n",
    ")\n",
    "# Create output directories\n",
    "output_dir = Path(cfg.paths.output_dir)\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "checkpoint_dir = Path(cfg.trainer.checkpoint_dir)\n",
    "checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save the config to output directory\n",
    "OmegaConf.save(config=cfg, f=output_dir / \"config_multiscale.yaml\", resolve=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e002cf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5191047386be472c9b4ca63251f6eca9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training multiscale:   0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/myaldiz/anaconda3/envs/spnf/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:312: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n",
      "  warnings.warn(\n",
      "W1209 22:27:52.427000 2071494 site-packages/torch/_logging/_internal.py:1199] [4/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored\n",
      "W1209 22:27:55.211000 2071494 site-packages/torch/_dynamo/convert_frame.py:1358] [8/8] torch._dynamo hit config.recompile_limit (8)\n",
      "W1209 22:27:55.211000 2071494 site-packages/torch/_dynamo/convert_frame.py:1358] [8/8]    function: 'scheduler' (/home/myaldiz/GitHub/Spectral-Prefiltering-of-Neural-Fields/spnf/utils.py:121)\n",
      "W1209 22:27:55.211000 2071494 site-packages/torch/_dynamo/convert_frame.py:1358] [8/8]    last reason: 8/7: (step / 100) == 0.08  # multiplier = np.clip(multiplier, 0.0, 1.0)  # GitHub/Spectral-Prefiltering-of-Neural-Fields/spnf/utils.py:149 in scheduler (_numpy/_util.py:177 in _try_convert_to_tensor)\n",
      "W1209 22:27:55.211000 2071494 site-packages/torch/_dynamo/convert_frame.py:1358] [8/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "W1209 22:27:55.211000 2071494 site-packages/torch/_dynamo/convert_frame.py:1358] [8/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rendering video frames...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "552b51c414c5416fb3cba025e64f1178",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/268 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved to /home/myaldiz/Data/Experiments/spnf/spnf-oia/visualizations/multiscale_train_vis.mp4\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(cfg).to(device)\n",
    "\n",
    "loss_average = []\n",
    "predictions = []\n",
    "steps_history = []  # New: Track the actual step number for the X-axis\n",
    "\n",
    "current_step = 0\n",
    "frame_count = 0\n",
    "pbar = tqdm(total=vis_steps, desc=\"Training multiscale\")\n",
    "\n",
    "while current_step < vis_steps:\n",
    "    # Determine stride based on how many frames we have generated\n",
    "    if current_step < 90:\n",
    "        stride = 1\n",
    "    elif current_step < 210:\n",
    "        stride = 2\n",
    "    else:\n",
    "        stride = 5\n",
    "        \n",
    "    # Ensure we don't exceed vis_steps\n",
    "    if current_step + stride > vis_steps:\n",
    "        stride = vis_steps - current_step\n",
    "        if stride == 0: break\n",
    "\n",
    "    # Fit for 'stride' steps\n",
    "    stats = trainer.fit(num_steps=stride, no_tqdm=True)\n",
    "    \n",
    "    # Update trackers\n",
    "    current_step += stride\n",
    "    frame_count += 1\n",
    "    pbar.update(stride)\n",
    "\n",
    "    # Store Data\n",
    "    loss_average.append(np.mean(stats['mse_loss']))\n",
    "    steps_history.append(current_step) # Store actual X-axis location\n",
    "    \n",
    "    # Generate visualization\n",
    "    output = trainer.generate_grid_data(resolution=SRC_RES)\n",
    "    predictions.append(output[\"filtered_signal\"].clamp(-1.0, 1.0).add(1.0).div(2.0).cpu().numpy())\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "# --- VIDEO GENERATION ---\n",
    "video_output = output_dir / \"visualizations\" / \"multiscale_train_vis.mp4\"\n",
    "video_output.parent.mkdir(parents=True, exist_ok=True)\n",
    "writer = imageio.get_writer(\n",
    "    video_output, \n",
    "    fps=30, \n",
    "    codec='libx264', \n",
    "    quality=None, \n",
    "    pixelformat='yuv420p',\n",
    "    ffmpeg_params=[\n",
    "        '-crf', '18', \n",
    "        '-preset', 'veryslow', \n",
    "        '-tune', 'grain'\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Settings\n",
    "plt.style.use('default') \n",
    "max_loss = max(loss_average) * 1.1\n",
    "\n",
    "# 1. Geometry Calculation\n",
    "sample = predictions[0]\n",
    "if sample.ndim == 3 and sample.shape[0] in [1, 3]:\n",
    "    h, w = sample.shape[1], sample.shape[2]\n",
    "else:\n",
    "    h, w = sample.shape[0], sample.shape[1]\n",
    "\n",
    "fig_width = 12 \n",
    "img_height_in = fig_width * (h / w)\n",
    "loss_height_in = 4.0 \n",
    "total_height = img_height_in + loss_height_in\n",
    "\n",
    "# Calculate relative height ratios for manual positioning\n",
    "h_img_ratio = img_height_in / total_height\n",
    "h_loss_ratio = loss_height_in / total_height\n",
    "\n",
    "print(\"Rendering video frames...\")\n",
    "# Zip with steps_history to get the correct X-axis value\n",
    "for i, (loss, pred, step_num) in enumerate(tqdm(zip(loss_average, predictions, steps_history), total=len(predictions))):\n",
    "    \n",
    "    # Create figure without subplots/layouts to allow manual placement\n",
    "    fig = plt.figure(figsize=(fig_width, total_height), dpi=100)\n",
    "    \n",
    "    # Bottom Plot\n",
    "    margin_bottom = 0.08\n",
    "    h_loss_actual = h_loss_ratio - margin_bottom - 0.02\n",
    "    ax_loss = fig.add_axes([0.12, margin_bottom, 0.82, h_loss_actual])\n",
    "    \n",
    "    margin_top = 0.02\n",
    "    y_img_start = h_loss_ratio + 0.02\n",
    "    h_img_actual = h_img_ratio - margin_top - 0.02\n",
    "    ax_img = fig.add_axes([0.02, y_img_start, 0.96, h_img_actual])\n",
    "\n",
    "    # --- 1. Image Plot ---\n",
    "    if pred.ndim == 3 and pred.shape[0] in [1, 3]: \n",
    "        pred = np.transpose(pred, (1, 2, 0))\n",
    "    \n",
    "    ax_img.imshow(pred, aspect='auto') \n",
    "    \n",
    "    # Hide all ticks/spines for the image\n",
    "    ax_img.set_xticks([])\n",
    "    ax_img.set_yticks([])\n",
    "    for spine in ax_img.spines.values():\n",
    "        spine.set_visible(False) \n",
    "    \n",
    "    # --- 2. Loss Plot ---\n",
    "    # MODIFICATION: Use steps_history for X-axis data\n",
    "    current_steps_data = steps_history[:i+1]\n",
    "    current_loss_data = loss_average[:i+1]\n",
    "    \n",
    "    ax_loss.plot(current_steps_data, current_loss_data, color='tab:blue', linewidth=3)\n",
    "    \n",
    "    # MODIFICATION: Use step_num (the actual training step) for the scatter X-coordinate\n",
    "    ax_loss.scatter(step_num, loss, color='tab:red', s=100, zorder=5) \n",
    "    \n",
    "    # MODIFICATION: Set X limit to the total vis_steps (800) rather than frame count\n",
    "    ax_loss.set_xlim(0, vis_steps)\n",
    "    ax_loss.set_ylim(0, max_loss)\n",
    "    \n",
    "    ax_loss.set_ylabel('MSE Loss', fontsize=24, labelpad=15, fontweight='medium')\n",
    "    ax_loss.set_xlabel('Steps', fontsize=18) # Optional: Label x-axis\n",
    "    ax_loss.tick_params(axis='both', which='major', labelsize=16)\n",
    "    ax_loss.grid(True, linestyle='--', alpha=0.5, linewidth=1.5)\n",
    "\n",
    "    # --- 3. Save Frame ---\n",
    "    fig.canvas.draw()\n",
    "    frame = np.array(fig.canvas.buffer_rgba())\n",
    "    \n",
    "    writer.append_data(frame)\n",
    "    plt.close(fig)\n",
    "\n",
    "writer.close()\n",
    "print(f\"Video saved to {video_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00137275",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcaa7df99ffc4377bc600792f18fabb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train remaining steps\n",
    "trainer.fit(num_steps=total_steps - vis_steps)\n",
    "\n",
    "# Save the state dict\n",
    "state_dict = trainer.state_dict()\n",
    "checkpoint_path = checkpoint_dir / \"multiscale.pth\"\n",
    "torch.save(state_dict, str(checkpoint_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8cf62f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rendering 300 frames...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61debea0af204b01b5e10c8c61a1466c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (3000, 1100) to (3008, 1104) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Saved /home/myaldiz/Data/Experiments/spnf/spnf-oia/visualizations/multiscale_aliasing_vis.mp4\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# ============ Visualization Script (Updated for Prefiltering) ============\n",
    "COLOR_BOX = '#D46CCD'\n",
    "COLOR_POINTS = '#4E71BE'\n",
    "SAMPLE_RES = 32\n",
    "FPS = 30\n",
    "SECONDS = 10\n",
    "FRAMES = FPS * SECONDS\n",
    "video_output = output_dir / \"visualizations\" / \"multiscale_aliasing_vis.mp4\"\n",
    "\n",
    "# --- Keyframes (Fixed Timing) ---\n",
    "# Format: (Time_Percent, Center_X, Center_Y, Span)\n",
    "keyframes = [\n",
    "    (0.00, -0.5,  0.5, 0.4), # Start Top-Left\n",
    "    (0.40,  0.5,  0.5, 0.4), # Move Right\n",
    "    (0.50,  0.5, -0.5, 1.0), # Zoom Out & Down\n",
    "    (0.90, -0.5, -0.5, 1.0), # Move Left (Zoomed Out)\n",
    "    (1.00, -0.5,  0.5, 0.4), # Zoom In & Up\n",
    "]\n",
    "\n",
    "def get_trajectory_point_time(t_global, keyframes):\n",
    "    for k in range(len(keyframes) - 1):\n",
    "        t0, x0, y0, s0 = keyframes[k]\n",
    "        t1, x1, y1, s1 = keyframes[k+1]\n",
    "        if t0 <= t_global <= t1 + 1e-5:\n",
    "            segment_duration = t1 - t0\n",
    "            if segment_duration <= 1e-5: return x1, y1, s1\n",
    "            local_t = np.clip((t_global - t0) / segment_duration, 0, 1)\n",
    "            smooth_t = local_t * local_t * (3 - 2 * local_t) \n",
    "            current_x = x0 + (x1 - x0) * smooth_t\n",
    "            current_y = y0 + (y1 - y0) * smooth_t\n",
    "            current_s = s0 + (s1 - s0) * smooth_t\n",
    "            return current_x, current_y, current_s\n",
    "    return keyframes[-1][1:]\n",
    "\n",
    "def clamp_bounds(cx, cy, span, limit=1.0):\n",
    "    half_s = span / 2.0\n",
    "    if cx - half_s < -limit: cx = -limit + half_s\n",
    "    if cx + half_s > limit:  cx = limit - half_s\n",
    "    if cy - half_s < -limit: cy = -limit + half_s\n",
    "    if cy + half_s > limit:  cy = limit - half_s\n",
    "    return cx, cy\n",
    "\n",
    "# --- Setup Plot (Now 1 Row, 3 Columns) ---\n",
    "plt.style.use('seaborn-v0_8-white') \n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5.5), dpi=200, constrained_layout=True)\n",
    "fig.patch.set_facecolor('white')\n",
    "\n",
    "# 1. Context View (Ground Truth)\n",
    "full_field = trainer.data\n",
    "output = trainer.generate_grid_data(resolution=SRC_RES)\n",
    "reconstructed_data = output[\"filtered_signal\"].clamp(-1.0, 1.0).add(1.0).div(2.0).cpu().numpy()\n",
    "im_context = axs[0].imshow(reconstructed_data, extent=(-1, 1, 1, -1), cmap='gray', vmin=0, vmax=1)\n",
    "axs[0].set_title(\"Learned Signal (Neural Field)\", fontsize=14, fontweight='bold', color='gray')\n",
    "axs[0].axis('off')\n",
    "\n",
    "rect = patches.Rectangle((0,0), 0, 0, linewidth=2.5, edgecolor=COLOR_BOX, facecolor='none')\n",
    "axs[0].add_patch(rect)\n",
    "scat = axs[0].scatter([], [], s=4, c=COLOR_POINTS, edgecolors='none', alpha=0.6)\n",
    "\n",
    "# Initialize dummy data for dynamic plots\n",
    "dummy_data = np.zeros((SAMPLE_RES, SAMPLE_RES, 3))\n",
    "\n",
    "# 2. Naive Sampling (Aliased)\n",
    "im_naive = axs[1].imshow(dummy_data, vmin=0, vmax=1, interpolation='nearest') \n",
    "axs[1].set_title(\"Naive Sampling (Aliased)\", fontsize=14, fontweight='bold', color=COLOR_BOX)\n",
    "axs[1].axis('off')\n",
    "\n",
    "# 3. Prefiltered (Ours) - New Column\n",
    "im_filtered = axs[2].imshow(dummy_data, vmin=0, vmax=1, interpolation='nearest')\n",
    "axs[2].set_title(\"Prefiltered Input (Ours)\", fontsize=14, fontweight='bold', color='tab:green')\n",
    "axs[2].axis('off')\n",
    "\n",
    "# --- Render Loop ---\n",
    "writer = imageio.get_writer(\n",
    "    video_output, \n",
    "    fps=FPS,\n",
    "    codec='libx264', \n",
    "    quality=None, \n",
    "    pixelformat='yuv420p',\n",
    "    ffmpeg_params=[\n",
    "        '-crf', '5', \n",
    "        '-preset', 'veryslow', \n",
    "        '-tune', 'grain'\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"Rendering {FRAMES} frames...\")\n",
    "\n",
    "for i in tqdm(range(FRAMES)):\n",
    "    t = i / (FRAMES - 1)\n",
    "    \n",
    "    # Logic\n",
    "    cx, cy, span = get_trajectory_point_time(t, keyframes)\n",
    "    cx, cy = clamp_bounds(cx, cy, span, limit=0.99)\n",
    "    bounds = (cx - span/2, cy - span/2, cx + span/2, cy + span/2)\n",
    "    \n",
    "    # Generate Coords\n",
    "    coords = make_coord_grid(ndim=2, resolution=SAMPLE_RES, bounds=bounds, device=device)\n",
    "    flattened_coords = coords.view(-1, 2)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # --- A. Naive Inference (Coords Only) ---\n",
    "        naive_out = trainer.model.forward(\n",
    "            {\"coords\": flattened_coords},\n",
    "        )\n",
    "        naive_out = naive_out[\"filtered_signal\"].view(SAMPLE_RES, SAMPLE_RES, 3)\n",
    "        naive_out = naive_out.clamp(-1.0, 1.0).add(1.0).div(2.0).cpu().numpy()\n",
    "\n",
    "        # --- B. Prefiltered Inference (Coords + Covariance) ---\n",
    "        # 1. Calculate pixel width based on current zoom (span)\n",
    "        pixel_width = span / SAMPLE_RES\n",
    "        \n",
    "        # 2. Construct isotropic covariance matrix: Diagonal(pixel_width^2)\n",
    "        # Shape: (N, 2, 2)\n",
    "        # Note: Depending on your specific kernel definition, you might scale this \n",
    "        # (e.g., pixel_width/2 for radius). Using width^2 is standard for variance.\n",
    "        # Match variance of a box filter: sigma^2 = width^2 / 12\n",
    "        cov_val = pixel_width ** 2 / 12.0\n",
    "        covariances = torch.eye(2, device=device).unsqueeze(0).repeat(flattened_coords.shape[0], 1, 1) * cov_val\n",
    "        \n",
    "        filtered_out = trainer.model.forward(\n",
    "            {\"coords\": flattened_coords, \"covariances\": covariances},\n",
    "        )\n",
    "        filtered_out = filtered_out[\"filtered_signal\"].view(SAMPLE_RES, SAMPLE_RES, 3)\n",
    "        filtered_out = filtered_out.clamp(-1.0, 1.0).add(1.0).div(2.0).cpu().numpy()\n",
    "\n",
    "    # Updates\n",
    "    rect.set_xy((bounds[0], bounds[1])) \n",
    "    rect.set_width(span)\n",
    "    rect.set_height(span)\n",
    "    \n",
    "    c_np = flattened_coords.cpu().numpy()\n",
    "    scat.set_offsets(c_np)\n",
    "    \n",
    "    # Image Updates\n",
    "    im_naive.set_data(naive_out)\n",
    "    im_filtered.set_data(filtered_out) # Update third column\n",
    "    \n",
    "    fig.canvas.draw()\n",
    "    frame = np.asarray(fig.canvas.buffer_rgba())\n",
    "    writer.append_data(frame)\n",
    "\n",
    "writer.close()\n",
    "plt.close()\n",
    "print(f\"Done. Saved {video_output}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spnf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
